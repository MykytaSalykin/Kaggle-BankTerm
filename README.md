# Bank Term Deposit â€” Binary Classification (Kaggle)

**Goal:** predict whether a client will subscribe to a term deposit after a marketing contact.  
**Metric:** ROCâ€“AUC (higher is better)

This repo provides a clean, CPU-friendly pipeline that is:
- **Reproducible** â€” deterministic CV and cached OOF/test predictions
- **Practical** â€” fast to train locally, simple to extend
- **Competitive** â€” strong single models + a simple stacker

---

## ğŸ” Whatâ€™s inside

- **Feature set**  
  Curated, competition-proven transforms:
  - clipped and log-scaled `duration`, `pdays` handling (`-1` â†’ â€œnot contactedâ€ flag + log on positives)
  - per-call intensity `duration_per_call`
  - cyclic encodings for `month`/`day`
  - simple interactions (e.g. `duration Ã— cellular`)
  - careful dtype management for native categorical support

- **Models**
  - **LightGBM (GBDT):** 3 seeds + a `scale_pos_weight` variant  
  - **CatBoost (CPU)** with native categorical handling  
  - **XGBoost (hist)** with one-hot for categoricals (via `OneHotEncoder`)
  - **Meta-learner:** Logistic Regression stacking on OOFs

- **Cross-validation**  
  `StratifiedKFold(n_splits=5, shuffle=True, random_state=42)`  
  OOF predictions are cached to `notebooks/outputs/cache/`.

---

## ğŸ“Š Current results (5-fold OOF AUC)

| Model               | OOF AUC  |
|---------------------|----------|
| LightGBM (seed=42)  | **0.97103** |
| LightGBM (seed=7)   | **0.97106** |
| LightGBM (+pos_w)   | 0.97060  |
| CatBoost (CPU)      | 0.96733  |
| XGBoost (OHE)       | 0.96922  |
| **Stack (LR on OOF)** | **0.97091** |

> Notes:
> - LightGBM singles are the strongest; stack is intentionally simple and transparent.
> - Scores are obtained on the provided `data/train.csv` with the feature set in `02_baselines.ipynb`.

---

### Public leaderboard (ensembling in `score_lab/`)

| Submission                  | LB Score | Notes                         |
|-----------------------------|----------|-------------------------------|
| **A2 mean-rank all**        | **0.97678** | Best public score (The 10% best) |
| Geometric mean (all models) | 0.97677  | Very close to best            |
| Diverse mean-rank top6      | 0.97636  | Slightly worse                |
| Weighted blends (C1)        | ~0.972   | Overfit, dropped              |

> âœ… Best LB score: **0.97678**.  
> Stacking + blind blending explored in `score_lab/`, but baselines remain clean and interpretable.


---

## ğŸš€ Quick start

1) **Data**  
Place files into `./data/`:
data/
â”œâ”€ train.csv
â””â”€ test.csv

2) **Environment**
```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

3) **Reproduce baselines (recommended)**
Open notebooks/02_baselines.ipynb
Run all cells â†’ this will train models and populate:
notebooks/outputs/cache/*_oof.npy
notebooks/outputs/cache/*_test.npy
notebooks/outputs/submissions/final_stack_*.csv

If cache already exists, you can regenerate only the final submission
```bash
python run_stack.py
```
Outputs appear in notebooks/outputs/submissions/.


ğŸ§  Why this setup

Determinism & speed. 
Everything fits on CPU and reproduces exactly thanks to fixed seeds and cached OOF.

Clarity > complexity. 
Strong baselines with clean feature logic beat fragile over-tuned stacks in most tabular comps.

Extendability. 
Add a new model? Just drop its OOF/TEST into cache and it instantly plugs into the stacker.


## ğŸ“ Project structure
â”œâ”€â”€ data/                          # train/test CSVs (not tracked)
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_eda.ipynb              # clean EDA with checks/drift/mutual info
â”‚   â”œâ”€â”€ 02_baselines.ipynb        # features + models + caching + stack
â”‚   â””â”€â”€ outputs/
â”‚       â”œâ”€â”€ cache/                # .npy OOF/TEST files
â”‚       â””â”€â”€ submissions/          # final CSVs for Kaggle
â”œâ”€â”€ score_lab/                    # extra ensembling "for leaderboard only"
â”‚   â”œâ”€â”€ pool/                     # external/public submissions
â”‚   â”œâ”€â”€ ensemble.py               # blending recipes
â”‚   â””â”€â”€ notes.md                  # LB tracking
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ run_stack.py                  # builds final stack from the cache
â””â”€â”€ README.md


ğŸ“œ License
MIT License â€” free to use and adapt.