{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a358920a",
   "metadata": {},
   "source": [
    "# 02 â€¢ Baselines and Stacking (reproducible from cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16caf1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, numpy as np, pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from datetime import datetime\n",
    "\n",
    "PROJ = Path.cwd()\n",
    "DATA = None\n",
    "for p in [PROJ/'data', PROJ.parent/'data', PROJ.parent.parent/'data']:\n",
    "    if (p/'train.csv').exists() and (p/'test.csv').exists():\n",
    "        DATA = p; break\n",
    "assert DATA is not None, 'data/train.csv or test.csv not found'\n",
    "\n",
    "OUT = PROJ/'notebooks'/'outputs'\n",
    "CACHE = OUT/'cache'\n",
    "SUB = OUT/'submissions'\n",
    "for d in [OUT, CACHE, SUB]: d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BASE_NUM = ['age','balance','day','duration','campaign','pdays','previous']\n",
    "BASE_CAT = ['job','marital','education','default','housing','loan','contact','month','poutcome']\n",
    "\n",
    "train = pd.read_csv(DATA/'train.csv')\n",
    "test  = pd.read_csv(DATA/'test.csv')\n",
    "y = train['y'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5fd8cc",
   "metadata": {},
   "source": [
    "## Feature engineering used in final blend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade894dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def month_to_num(m):\n",
    "    d={'jan':1,'feb':2,'mar':3,'apr':4,'may':5,'jun':6,'jul':7,'aug':8,'sep':9,'oct':10,'nov':11,'dec':12}\n",
    "    return d.get(m,0)\n",
    "\n",
    "def make_curated_features(df):\n",
    "    out = df.copy()\n",
    "    out['duration_clip_99'] = np.minimum(out['duration'], out['duration'].quantile(0.99))\n",
    "    out['duration_log1p'] = np.log1p(out['duration_clip_99'])\n",
    "    out['duration_per_call'] = out['duration_clip_99'] / (out['campaign'] + 1.0)\n",
    "    out['pdays_was_contacted'] = (out['pdays'] != -1).astype(int)\n",
    "    out['pdays_pos_log'] = np.log1p(out['pdays'].where(out['pdays'] != -1, np.nan)).fillna(0.0)\n",
    "    out['previous_gt0'] = (out['previous'] > 0).astype(int)\n",
    "    out['month_num'] = out['month'].map(month_to_num).astype(int)\n",
    "    out['month_sin'] = np.sin(2*np.pi*out['month_num']/12.0)\n",
    "    out['month_cos'] = np.cos(2*np.pi*out['month_num']/12.0)\n",
    "    out['day_sin'] = np.sin(2*np.pi*out['day']/31.0)\n",
    "    out['day_cos'] = np.cos(2*np.pi*out['day']/31.0)\n",
    "    out['contact_cellular'] = (out['contact'] == 'cellular').astype(int)\n",
    "    out['dur_x_cell'] = out['duration_clip_99'] * out['contact_cellular']\n",
    "    return out\n",
    "\n",
    "train_f = make_curated_features(train.drop(columns=['id']))\n",
    "test_f  = make_curated_features(test.drop(columns=['id']))\n",
    "X_feat = train_f.drop(columns=['y']).copy()\n",
    "X_test_feat = test_f.copy()\n",
    "for c in BASE_CAT:\n",
    "    if c in X_feat.columns: X_feat[c] = X_feat[c].astype('category')\n",
    "    if c in X_test_feat.columns: X_test_feat[c] = X_test_feat[c].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e98a9e",
   "metadata": {},
   "source": [
    "## CV helpers and cache IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a8a663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cache(name, oof, pred, cache_dir=None):\n",
    "    cache_dir = (cache_dir or (Path.cwd()/'notebooks'/'outputs'/'cache'))\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    np.save(cache_dir/f\"{name}_oof.npy\", oof)\n",
    "    np.save(cache_dir/f\"{name}_test.npy\", pred)\n",
    "\n",
    "def load_cache(name, cache_dir=None):\n",
    "    cache_dir = (cache_dir or (Path.cwd()/'notebooks'/'outputs'/'cache'))\n",
    "    o = np.load(cache_dir/f\"{name}_oof.npy\")\n",
    "    t = np.load(cache_dir/f\"{name}_test.npy\")\n",
    "    return o, t\n",
    "\n",
    "def has_cache(name, cache_dir=None):\n",
    "    cache_dir = (cache_dir or (Path.cwd()/'notebooks'/'outputs'/'cache'))\n",
    "    return (cache_dir/f\"{name}_oof.npy\").exists() and (cache_dir/f\"{name}_test.npy\").exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49b96ad",
   "metadata": {},
   "source": [
    "## LightGBM (3 seeds + pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46619ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def cv_lgbm(X_tr_all, y_all, X_te_all, params, n_splits=5, seed=42, num_boost_round=3500, es_rounds=250, cats=BASE_CAT):\n",
    "    oof = np.zeros(len(X_tr_all)); pred = np.zeros(len(X_te_all))\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    use_es = params.get('boosting_type','gbdt')!='dart'\n",
    "    for tr, va in skf.split(X_tr_all, y_all):\n",
    "        X_tr, X_va = X_tr_all.iloc[tr], X_tr_all.iloc[va]\n",
    "        y_tr, y_va = y_all[tr], y_all[va]\n",
    "        dtr = lgb.Dataset(X_tr, label=y_tr, categorical_feature=cats, free_raw_data=False)\n",
    "        dva = lgb.Dataset(X_va, label=y_va, categorical_feature=cats, free_raw_data=False)\n",
    "        cbs=[lgb.log_evaluation(period=200)]\n",
    "        if use_es: cbs.append(lgb.early_stopping(stopping_rounds=es_rounds))\n",
    "        m = lgb.train(params, dtr, valid_sets=[dtr,dva], valid_names=['train','valid'],\n",
    "                      num_boost_round=num_boost_round, callbacks=cbs)\n",
    "        best_iter = m.best_iteration if use_es else num_boost_round\n",
    "        oof[va] = m.predict(X_va, num_iteration=best_iter)\n",
    "        pred += m.predict(X_te_all, num_iteration=best_iter)/n_splits\n",
    "    return oof, pred\n",
    "\n",
    "pos_weight = (y==0).sum()/(y==1).sum()\n",
    "def make_params(seed, spw=None):\n",
    "    p = {\n",
    "        'objective':'binary','metric':'auc','boosting_type':'gbdt',\n",
    "        'learning_rate':0.03,'num_leaves':127,'min_data_in_leaf':96,\n",
    "        'feature_fraction':0.85,'bagging_fraction':0.85,'bagging_freq':1,\n",
    "        'min_sum_hessian_in_leaf':5.0,'lambda_l2':10.0,'max_bin':511,\n",
    "        'seed':seed,'n_jobs':-1,'verbosity':-1,'force_row_wise':True\n",
    "    }\n",
    "    if spw is not None: p['scale_pos_weight'] = spw\n",
    "    return p\n",
    "\n",
    "if not has_cache('lgbF_s42'):\n",
    "    o1,t1 = cv_lgbm(X_feat, y, X_test_feat, make_params(42), num_boost_round=3200, es_rounds=250)\n",
    "    save_cache('lgbF_s42', o1, t1)\n",
    "if not has_cache('lgbF_s7'):\n",
    "    o2,t2 = cv_lgbm(X_feat, y, X_test_feat, make_params(7), num_boost_round=3200, es_rounds=250)\n",
    "    save_cache('lgbF_s7', o2, t2)\n",
    "if not has_cache('lgbF_spw'):\n",
    "    o3,t3 = cv_lgbm(X_feat, y, X_test_feat, make_params(2025, pos_weight), num_boost_round=3200, es_rounds=250)\n",
    "    save_cache('lgbF_spw', o3, t3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c779cffd",
   "metadata": {},
   "source": [
    "## CatBoost (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e3d2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "def cv_cat_cpu_feat(Xdf, yarr, Xte, cat_cols, n_splits=5, seed=42):\n",
    "    oof = np.zeros(len(Xdf)); pred = np.zeros(len(Xte))\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    for fold,(tr,va) in enumerate(skf.split(Xdf,yarr),1):\n",
    "        X_tr, X_va = Xdf.iloc[tr], Xdf.iloc[va]\n",
    "        y_tr, y_va = yarr[tr], yarr[va]\n",
    "        trp = Pool(X_tr, y_tr, cat_features=cat_cols)\n",
    "        vap = Pool(X_va, y_va, cat_features=cat_cols)\n",
    "        tep = Pool(Xte,           cat_features=cat_cols)\n",
    "        m = CatBoostClassifier(iterations=2500, depth=6, learning_rate=0.05, l2_leaf_reg=6,\n",
    "                               loss_function='Logloss', eval_metric='AUC', random_seed=seed+fold,\n",
    "                               od_type='Iter', od_wait=200, verbose=200, task_type='CPU',\n",
    "                               thread_count=-1, allow_writing_files=False)\n",
    "        m.fit(trp, eval_set=vap, use_best_model=True)\n",
    "        oof[va] = m.predict_proba(vap)[:,1]\n",
    "        pred += m.predict_proba(tep)[:,1]/n_splits\n",
    "    return oof, pred\n",
    "\n",
    "if not has_cache('catF_cpu'):\n",
    "    oc, tc = cv_cat_cpu_feat(X_feat, y, X_test_feat, BASE_CAT)\n",
    "    save_cache('catF_cpu', oc, tc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d20bd35",
   "metadata": {},
   "source": [
    "## XGBoost with OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2308c2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def _make_ohe():\n",
    "    try: return OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    except TypeError: return OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "def cv_xgb_ohe_native(Xdf, yarr, Xte, base_cat, n_splits=5, seed=42, es_rounds=400, num_boost_round=6000):\n",
    "    num_base = [c for c in Xdf.columns if c not in base_cat]\n",
    "    oof = np.zeros(len(Xdf)); pred = np.zeros(len(Xte))\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    for fold,(tr,va) in enumerate(skf.split(Xdf,yarr),1):\n",
    "        X_tr, X_va = Xdf.iloc[tr].copy(), Xdf.iloc[va].copy()\n",
    "        y_tr, y_va = yarr[tr], yarr[va]\n",
    "        X_te = Xte.copy()\n",
    "        ohe = _make_ohe()\n",
    "        Xtr = np.hstack([X_tr[num_base].to_numpy(dtype=np.float32),\n",
    "                         ohe.fit_transform(X_tr[base_cat].astype(str)).astype(np.float32)])\n",
    "        Xva = np.hstack([X_va[num_base].to_numpy(dtype=np.float32),\n",
    "                         ohe.transform(X_va[base_cat].astype(str)).astype(np.float32)])\n",
    "        Xtt = np.hstack([X_te[num_base].to_numpy(dtype=np.float32),\n",
    "                         ohe.transform(X_te[base_cat].astype(str)).astype(np.float32)])\n",
    "        dtr = xgb.DMatrix(Xtr, label=y_tr); dva = xgb.DMatrix(Xva, label=y_va); dte = xgb.DMatrix(Xtt)\n",
    "        params = {'objective':'binary:logistic','eval_metric':'auc','eta':0.03,'max_depth':6,\n",
    "                  'subsample':0.8,'colsample_bytree':0.8,'lambda':5.0,'alpha':0.0,\n",
    "                  'tree_method':'hist','seed':seed+fold}\n",
    "        es = xgb.callback.EarlyStopping(rounds=es_rounds, save_best=True, maximize=True)\n",
    "        bst = xgb.train(params, dtr, num_boost_round=num_boost_round, evals=[(dtr,'train'),(dva,'valid')],\n",
    "                        callbacks=[es], verbose_eval=False)\n",
    "        oof[va] = bst.predict(dva)\n",
    "        pred += bst.predict(dte)/n_splits\n",
    "    return oof, pred\n",
    "\n",
    "if not has_cache('xgbF_ohe'):\n",
    "    ox, tx = cv_xgb_ohe_native(X_feat, y, X_test_feat, BASE_CAT, n_splits=5, seed=42, es_rounds=400, num_boost_round=6000)\n",
    "    save_cache('xgbF_ohe', ox, tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcbc14b",
   "metadata": {},
   "source": [
    "## Report OOF AUC per model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452be07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "o1,_=load_cache('lgbF_s42'); o2,_=load_cache('lgbF_s7'); o3,_=load_cache('lgbF_spw'); oc,_=load_cache('catF_cpu'); ox,_=load_cache('xgbF_ohe')\n",
    "print({\n",
    "    'lgbF_s42': float(roc_auc_score(y,o1)),\n",
    "    'lgbF_s7': float(roc_auc_score(y,o2)),\n",
    "    'lgbF_spw': float(roc_auc_score(y,o3)),\n",
    "    'catF_cpu': float(roc_auc_score(y,oc)),\n",
    "    'xgbF_ohe': float(roc_auc_score(y,ox)),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8652fff7",
   "metadata": {},
   "source": [
    "## Stacking (Logistic Regression) and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f987f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "o1,t1 = load_cache('lgbF_s42')\n",
    "o2,t2 = load_cache('lgbF_s7')\n",
    "o3,t3 = load_cache('lgbF_spw')\n",
    "oc,tc = load_cache('catF_cpu')\n",
    "ox,tx = load_cache('xgbF_ohe')\n",
    "\n",
    "Z_tr = np.vstack([o1,o2,o3,oc,ox]).T\n",
    "Z_te = np.vstack([t1,t2,t3,tc,tx]).T\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_meta = np.zeros(len(y)); pred_meta = np.zeros(len(test))\n",
    "for tr, va in skf.split(Z_tr, y):\n",
    "    m = LogisticRegression(max_iter=1000)\n",
    "    m.fit(Z_tr[tr], y[tr])\n",
    "    oof_meta[va] = m.predict_proba(Z_tr[va])[:,1]\n",
    "    pred_meta += m.predict_proba(Z_te)[:,1]/skf.n_splits\n",
    "\n",
    "print('Stack OOF AUC:', float(roc_auc_score(y, oof_meta)))\n",
    "ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "sub_path = (PROJ/'notebooks'/'outputs'/'submissions'/f'final_stack_{ts}.csv')\n",
    "pd.DataFrame({'id': pd.read_csv(DATA/'test.csv')['id'], 'y': pred_meta}).to_csv(sub_path, index=False)\n",
    "print('Saved:', sub_path)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
