{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR: /Users/mykytasalykin/Desktop/kaggle-bankterm/kaggle-bankterm/data\n",
      "OOF AUC: 0.9427161029459289\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "cwd = Path.cwd()\n",
    "\n",
    "candidates = [\n",
    "    cwd / \"data\",             \n",
    "    cwd.parent / \"data\",     \n",
    "    cwd.parent.parent / \"data\"  \n",
    "]\n",
    "\n",
    "DATA_DIR = next((p for p in candidates if (p / \"train.csv\").exists()), None)\n",
    "if DATA_DIR is None:\n",
    "    raise FileNotFoundError(f\"Не нашёл data/ рядом с ноутбуком. Проверил: {', '.join(str(p) for p in candidates)}\")\n",
    "\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "train = pd.read_csv(DATA_DIR / \"train.csv\")\n",
    "test  = pd.read_csv(DATA_DIR / \"test.csv\")\n",
    "\n",
    "y = train[\"y\"]\n",
    "X = train.drop(columns=[\"y\", \"id\"])\n",
    "X_test = test.drop(columns=[\"id\"])\n",
    "\n",
    "num_cols = [\"age\", \"balance\", \"day\", \"duration\", \"campaign\", \"pdays\", \"previous\"]\n",
    "cat_cols = [\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"contact\", \"month\", \"poutcome\"]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", \"passthrough\", num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", LogisticRegression(max_iter=1000, class_weight=\"balanced\", solver=\"liblinear\"))\n",
    "])\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_preds = []\n",
    "oof_y = []\n",
    "\n",
    "for train_idx, val_idx in cv.split(X, y):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict_proba(X_val)[:, 1]\n",
    "    oof_preds.extend(preds)\n",
    "    oof_y.extend(y_val)\n",
    "\n",
    "print(\"OOF AUC:\", roc_auc_score(oof_y, oof_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016260 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1009\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n",
      "[LightGBM] [Info] Start training from score -1.986273\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttrain's auc: 0.969137\tvalid's auc: 0.967272\n",
      "[400]\ttrain's auc: 0.972649\tvalid's auc: 0.968318\n",
      "[600]\ttrain's auc: 0.975182\tvalid's auc: 0.968749\n",
      "[800]\ttrain's auc: 0.977288\tvalid's auc: 0.968956\n",
      "[1000]\ttrain's auc: 0.979097\tvalid's auc: 0.969039\n",
      "[1200]\ttrain's auc: 0.980795\tvalid's auc: 0.969167\n",
      "[1400]\ttrain's auc: 0.982305\tvalid's auc: 0.969194\n",
      "Early stopping, best iteration is:\n",
      "[1386]\ttrain's auc: 0.982212\tvalid's auc: 0.969215\n",
      "[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009470 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1007\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n",
      "[LightGBM] [Info] Start training from score -1.986273\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttrain's auc: 0.96937\tvalid's auc: 0.966229\n",
      "[400]\ttrain's auc: 0.972845\tvalid's auc: 0.967297\n",
      "[600]\ttrain's auc: 0.975397\tvalid's auc: 0.967784\n",
      "[800]\ttrain's auc: 0.977533\tvalid's auc: 0.967977\n",
      "[1000]\ttrain's auc: 0.979397\tvalid's auc: 0.968077\n",
      "[1200]\ttrain's auc: 0.981053\tvalid's auc: 0.968134\n",
      "[1400]\ttrain's auc: 0.982565\tvalid's auc: 0.968118\n",
      "Early stopping, best iteration is:\n",
      "[1248]\ttrain's auc: 0.981424\tvalid's auc: 0.968154\n",
      "[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014650 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1005\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n",
      "[LightGBM] [Info] Start training from score -1.986289\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttrain's auc: 0.969432\tvalid's auc: 0.966318\n",
      "[400]\ttrain's auc: 0.972884\tvalid's auc: 0.967342\n",
      "[600]\ttrain's auc: 0.975384\tvalid's auc: 0.967679\n",
      "[800]\ttrain's auc: 0.977527\tvalid's auc: 0.967813\n",
      "[1000]\ttrain's auc: 0.979391\tvalid's auc: 0.967924\n",
      "[1200]\ttrain's auc: 0.981046\tvalid's auc: 0.967909\n",
      "Early stopping, best iteration is:\n",
      "[1177]\ttrain's auc: 0.980867\tvalid's auc: 0.967928\n",
      "[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010104 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1002\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n",
      "[LightGBM] [Info] Start training from score -1.986289\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttrain's auc: 0.969154\tvalid's auc: 0.967197\n",
      "[400]\ttrain's auc: 0.972682\tvalid's auc: 0.96832\n",
      "[600]\ttrain's auc: 0.975229\tvalid's auc: 0.96878\n",
      "[800]\ttrain's auc: 0.977287\tvalid's auc: 0.968951\n",
      "[1000]\ttrain's auc: 0.979176\tvalid's auc: 0.969061\n",
      "[1200]\ttrain's auc: 0.98085\tvalid's auc: 0.969106\n",
      "[1400]\ttrain's auc: 0.982385\tvalid's auc: 0.969162\n",
      "[1600]\ttrain's auc: 0.983805\tvalid's auc: 0.969212\n",
      "Early stopping, best iteration is:\n",
      "[1561]\ttrain's auc: 0.983536\tvalid's auc: 0.969221\n",
      "[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011864 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1006\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n",
      "[LightGBM] [Info] Start training from score -1.986289\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttrain's auc: 0.969258\tvalid's auc: 0.96648\n",
      "[400]\ttrain's auc: 0.972765\tvalid's auc: 0.967636\n",
      "[600]\ttrain's auc: 0.975255\tvalid's auc: 0.967995\n",
      "[800]\ttrain's auc: 0.97738\tvalid's auc: 0.968252\n",
      "[1000]\ttrain's auc: 0.979226\tvalid's auc: 0.968337\n",
      "[1200]\ttrain's auc: 0.980881\tvalid's auc: 0.968398\n",
      "[1400]\ttrain's auc: 0.982414\tvalid's auc: 0.968427\n",
      "Early stopping, best iteration is:\n",
      "[1308]\ttrain's auc: 0.981742\tvalid's auc: 0.968441\n",
      "OOF AUC: 0.968590269806596\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y = train[\"y\"]\n",
    "X = train.drop(columns=[\"y\", \"id\"]).copy()\n",
    "X_test = test.drop(columns=[\"id\"]).copy()\n",
    "\n",
    "num_cols = [\"age\", \"balance\", \"day\", \"duration\", \"campaign\", \"pdays\", \"previous\"]\n",
    "cat_cols = [\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"contact\", \"month\", \"poutcome\"]\n",
    "\n",
    "for c in cat_cols:\n",
    "    X[c] = X[c].astype(\"category\")\n",
    "    X_test[c] = X_test[c].astype(\"category\")\n",
    "\n",
    "pos_weight = (y == 0).sum() / (y == 1).sum()\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"num_leaves\": 63,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"scale_pos_weight\": pos_weight,\n",
    "    \"seed\": 42,\n",
    "    \"n_jobs\": -1,\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof = np.zeros(len(X))\n",
    "test_pred = np.zeros(len(X_test))\n",
    "\n",
    "for tr_idx, va_idx in cv.split(X, y):\n",
    "    X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "    y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "\n",
    "    dtr = lgb.Dataset(X_tr, label=y_tr, categorical_feature=cat_cols, free_raw_data=False)\n",
    "    dva = lgb.Dataset(X_va, label=y_va, categorical_feature=cat_cols, free_raw_data=False)\n",
    "\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        dtr,\n",
    "        valid_sets=[dtr, dva],\n",
    "        valid_names=[\"train\", \"valid\"],\n",
    "        num_boost_round=5000,\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=200),\n",
    "            lgb.log_evaluation(period=200),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    oof[va_idx] = model.predict(X_va, num_iteration=model.best_iteration)\n",
    "    test_pred += model.predict(X_test, num_iteration=model.best_iteration) / cv.n_splits\n",
    "\n",
    "print(\"OOF AUC:\", roc_auc_score(y, oof))\n",
    "\n",
    "from pathlib import Path\n",
    "Path(\"outputs/submissions\").mkdir(parents=True, exist_ok=True)\n",
    "pd.DataFrame({\"id\": test[\"id\"], \"y\": test_pred}).to_csv(\"outputs/submissions/lgbm_baseline.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def month_to_num(m):\n",
    "    mapping = {'jan':1,'feb':2,'mar':3,'apr':4,'may':5,'jun':6,'jul':7,'aug':8,'sep':9,'oct':10,'nov':11,'dec':12}\n",
    "    return mapping.get(m, 0)\n",
    "\n",
    "def make_features(df):\n",
    "    out = df.copy()\n",
    "    out['balance_is_neg'] = (out['balance'] < 0).astype(int)\n",
    "    out['balance_log1p'] = np.log1p(np.maximum(out['balance'], 0))\n",
    "    q99 = out['duration'].quantile(0.99)\n",
    "    out['duration_clip_99'] = np.minimum(out['duration'], q99)\n",
    "    out['duration_log1p'] = np.log1p(out['duration_clip_99'])\n",
    "    out['duration_per_call'] = out['duration_clip_99'] / (out['campaign'] + 1.0)\n",
    "    out['pdays_was_contacted'] = (out['pdays'] != -1).astype(int)\n",
    "    out['pdays_pos'] = out['pdays'].where(out['pdays'] != -1, np.nan)\n",
    "    out['previous_gt0'] = (out['previous'] > 0).astype(int)\n",
    "    q90 = out['campaign'].quantile(0.90)\n",
    "    out['campaign_high_q90'] = (out['campaign'] >= q90).astype(int)\n",
    "    out['month_num'] = out['month'].map(month_to_num).astype(int)\n",
    "    out['month_sin'] = np.sin(2*np.pi*out['month_num']/12.0)\n",
    "    out['month_cos'] = np.cos(2*np.pi*out['month_num']/12.0)\n",
    "    out['day_sin'] = np.sin(2*np.pi*out['day']/31.0)\n",
    "    out['day_cos'] = np.cos(2*np.pi*out['day']/31.0)\n",
    "    out['contact_cellular'] = (out['contact'] == 'cellular').astype(int)\n",
    "    out['housing_yes'] = (out['housing'] == 'yes').astype(int)\n",
    "    out['loan_yes'] = (out['loan'] == 'yes').astype(int)\n",
    "    out['default_yes'] = (out['default'] == 'yes').astype(int)\n",
    "    out['dur_x_cell'] = out['duration_clip_99'] * out['contact_cellular']\n",
    "    out['dur_x_prevpos'] = out['duration_clip_99'] * out['previous_gt0']\n",
    "    return out\n",
    "\n",
    "train_f = make_features(train)\n",
    "test_f = make_features(test)\n",
    "\n",
    "y = train_f['y'].values\n",
    "drop_cols = ['y','id']\n",
    "X = train_f.drop(columns=drop_cols)\n",
    "X_test = test_f.drop(columns=['id'])\n",
    "\n",
    "cat_cols = [\"job\",\"marital\",\"education\",\"default\",\"housing\",\"loan\",\"contact\",\"month\",\"poutcome\"]\n",
    "for c in cat_cols:\n",
    "    X[c] = X[c].astype('category')\n",
    "    X_test[c] = X_test[c].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_target_encode(train_df, test_df, cols, target, n_splits=5, seed=42, suffix=\"_te\"):\n",
    "    te_train = pd.DataFrame(index=train_df.index)\n",
    "    te_test = pd.DataFrame(index=test_df.index)\n",
    "    global_means = {}\n",
    "    for c in cols:\n",
    "        global_means[c] = train_df.groupby(c)[target].mean()\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    for c in cols:\n",
    "        te_col = np.zeros(len(train_df))\n",
    "        for tr_idx, va_idx in skf.split(train_df, train_df[target]):\n",
    "            tr = train_df.iloc[tr_idx]; va = train_df.iloc[va_idx]\n",
    "            means = tr.groupby(c)[target].mean()\n",
    "            te_col[va_idx] = va[c].map(means).fillna(train_df[target].mean()).values\n",
    "        te_train[c + suffix] = te_col\n",
    "        te_test[c + suffix] = test_df[c].map(train_df.groupby(c)[target].mean()).fillna(train_df[target].mean()).values\n",
    "    return te_train, te_test\n",
    "\n",
    "te_cols = [\"job\",\"education\",\"contact\",\"month\",\"poutcome\"]\n",
    "te_train, te_test = kfold_target_encode(train_f, test_f, te_cols, target='y', n_splits=5, seed=42)\n",
    "\n",
    "X_te = pd.concat([X.reset_index(drop=True), te_train.reset_index(drop=True)], axis=1)\n",
    "X_test_te = pd.concat([X_test.reset_index(drop=True), te_test.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023537 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3042\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n",
      "[LightGBM] [Info] Start training from score -1.986273\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[200]\ttrain's auc: 0.969369\tvalid's auc: 0.966888\n",
      "[400]\ttrain's auc: 0.973957\tvalid's auc: 0.968911\n",
      "[600]\ttrain's auc: 0.976792\tvalid's auc: 0.969503\n",
      "[800]\ttrain's auc: 0.979076\tvalid's auc: 0.969851\n",
      "[1000]\ttrain's auc: 0.981036\tvalid's auc: 0.970046\n",
      "[1200]\ttrain's auc: 0.982774\tvalid's auc: 0.970178\n",
      "[1400]\ttrain's auc: 0.984228\tvalid's auc: 0.970205\n",
      "[1600]\ttrain's auc: 0.98557\tvalid's auc: 0.970222\n",
      "[1800]\ttrain's auc: 0.986846\tvalid's auc: 0.970237\n",
      "[2000]\ttrain's auc: 0.987964\tvalid's auc: 0.970231\n",
      "[2200]\ttrain's auc: 0.989004\tvalid's auc: 0.970227\n",
      "Early stopping, best iteration is:\n",
      "[1923]\ttrain's auc: 0.987547\tvalid's auc: 0.970241\n",
      "[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034723 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3040\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n",
      "[LightGBM] [Info] Start training from score -1.986273\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[200]\ttrain's auc: 0.969565\tvalid's auc: 0.9658\n",
      "[400]\ttrain's auc: 0.974301\tvalid's auc: 0.967989\n",
      "[600]\ttrain's auc: 0.97713\tvalid's auc: 0.968591\n",
      "[800]\ttrain's auc: 0.979348\tvalid's auc: 0.968813\n",
      "[1000]\ttrain's auc: 0.981242\tvalid's auc: 0.968995\n",
      "[1200]\ttrain's auc: 0.982908\tvalid's auc: 0.969069\n",
      "[1400]\ttrain's auc: 0.984424\tvalid's auc: 0.969092\n",
      "[1600]\ttrain's auc: 0.985795\tvalid's auc: 0.969103\n",
      "[1800]\ttrain's auc: 0.987027\tvalid's auc: 0.969125\n",
      "[2000]\ttrain's auc: 0.988119\tvalid's auc: 0.969134\n",
      "[2200]\ttrain's auc: 0.989149\tvalid's auc: 0.969132\n",
      "[2400]\ttrain's auc: 0.990049\tvalid's auc: 0.969106\n",
      "Early stopping, best iteration is:\n",
      "[2247]\ttrain's auc: 0.989365\tvalid's auc: 0.969139\n",
      "[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018755 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3038\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n",
      "[LightGBM] [Info] Start training from score -1.986289\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[200]\ttrain's auc: 0.969602\tvalid's auc: 0.965836\n",
      "[400]\ttrain's auc: 0.974319\tvalid's auc: 0.968029\n",
      "[600]\ttrain's auc: 0.977035\tvalid's auc: 0.968531\n",
      "[800]\ttrain's auc: 0.979284\tvalid's auc: 0.968825\n",
      "[1000]\ttrain's auc: 0.981186\tvalid's auc: 0.968974\n",
      "[1200]\ttrain's auc: 0.982931\tvalid's auc: 0.969081\n",
      "[1400]\ttrain's auc: 0.984442\tvalid's auc: 0.969146\n",
      "[1600]\ttrain's auc: 0.985805\tvalid's auc: 0.969165\n",
      "[1800]\ttrain's auc: 0.987032\tvalid's auc: 0.969148\n",
      "Early stopping, best iteration is:\n",
      "[1642]\ttrain's auc: 0.986081\tvalid's auc: 0.969169\n",
      "[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018091 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3035\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n",
      "[LightGBM] [Info] Start training from score -1.986289\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[200]\ttrain's auc: 0.969447\tvalid's auc: 0.966875\n",
      "[400]\ttrain's auc: 0.974018\tvalid's auc: 0.96891\n",
      "[600]\ttrain's auc: 0.976828\tvalid's auc: 0.969493\n",
      "[800]\ttrain's auc: 0.979126\tvalid's auc: 0.969773\n",
      "[1000]\ttrain's auc: 0.981085\tvalid's auc: 0.96992\n",
      "[1200]\ttrain's auc: 0.982829\tvalid's auc: 0.970032\n",
      "[1400]\ttrain's auc: 0.984336\tvalid's auc: 0.970123\n",
      "[1600]\ttrain's auc: 0.98571\tvalid's auc: 0.970125\n",
      "[1800]\ttrain's auc: 0.986968\tvalid's auc: 0.970162\n",
      "[2000]\ttrain's auc: 0.988085\tvalid's auc: 0.970149\n",
      "[2200]\ttrain's auc: 0.989085\tvalid's auc: 0.970136\n",
      "Early stopping, best iteration is:\n",
      "[1926]\ttrain's auc: 0.987677\tvalid's auc: 0.970166\n",
      "[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025251 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3039\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n",
      "[LightGBM] [Info] Start training from score -1.986289\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[200]\ttrain's auc: 0.969649\tvalid's auc: 0.966203\n",
      "[400]\ttrain's auc: 0.974298\tvalid's auc: 0.96827\n",
      "[600]\ttrain's auc: 0.977029\tvalid's auc: 0.968822\n",
      "[800]\ttrain's auc: 0.979315\tvalid's auc: 0.969088\n",
      "[1000]\ttrain's auc: 0.981215\tvalid's auc: 0.969269\n",
      "[1200]\ttrain's auc: 0.98289\tvalid's auc: 0.969372\n",
      "[1400]\ttrain's auc: 0.984402\tvalid's auc: 0.969432\n",
      "[1600]\ttrain's auc: 0.985784\tvalid's auc: 0.96947\n",
      "[1800]\ttrain's auc: 0.987022\tvalid's auc: 0.969498\n",
      "[2000]\ttrain's auc: 0.988127\tvalid's auc: 0.969513\n",
      "[2200]\ttrain's auc: 0.989139\tvalid's auc: 0.969519\n",
      "Early stopping, best iteration is:\n",
      "[2042]\ttrain's auc: 0.988358\tvalid's auc: 0.969529\n",
      "[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024959 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3042\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n",
      "[LightGBM] [Info] Start training from score -1.986273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mykytasalykin/Desktop/kaggle-bankterm/.venv/lib/python3.12/site-packages/lightgbm/callback.py:333: UserWarning: Early stopping is not available in dart mode\n",
      "  _log_warning(\"Early stopping is not available in dart mode\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttrain's auc: 0.961993\tvalid's auc: 0.961096\n",
      "[400]\ttrain's auc: 0.964632\tvalid's auc: 0.96345\n",
      "[600]\ttrain's auc: 0.966684\tvalid's auc: 0.965003\n",
      "[800]\ttrain's auc: 0.96947\tvalid's auc: 0.966885\n",
      "[1000]\ttrain's auc: 0.971313\tvalid's auc: 0.967897\n",
      "[1200]\ttrain's auc: 0.972743\tvalid's auc: 0.968574\n",
      "[1400]\ttrain's auc: 0.973787\tvalid's auc: 0.96891\n",
      "[1600]\ttrain's auc: 0.974982\tvalid's auc: 0.969298\n",
      "[1800]\ttrain's auc: 0.975923\tvalid's auc: 0.969534\n",
      "[2000]\ttrain's auc: 0.976796\tvalid's auc: 0.969688\n",
      "[2200]\ttrain's auc: 0.977478\tvalid's auc: 0.969796\n",
      "[2400]\ttrain's auc: 0.978195\tvalid's auc: 0.969912\n",
      "[2600]\ttrain's auc: 0.978884\tvalid's auc: 0.970027\n",
      "[2800]\ttrain's auc: 0.979533\tvalid's auc: 0.970079\n",
      "[3000]\ttrain's auc: 0.980264\tvalid's auc: 0.970198\n",
      "[3200]\ttrain's auc: 0.980923\tvalid's auc: 0.970284\n",
      "[3400]\ttrain's auc: 0.981587\tvalid's auc: 0.970331\n",
      "[3600]\ttrain's auc: 0.982213\tvalid's auc: 0.970409\n",
      "[3800]\ttrain's auc: 0.98289\tvalid's auc: 0.970455\n",
      "[4000]\ttrain's auc: 0.983363\tvalid's auc: 0.970479\n",
      "[4200]\ttrain's auc: 0.983931\tvalid's auc: 0.970482\n",
      "[4400]\ttrain's auc: 0.984469\tvalid's auc: 0.970486\n",
      "[4600]\ttrain's auc: 0.985025\tvalid's auc: 0.970513\n",
      "[4800]\ttrain's auc: 0.9855\tvalid's auc: 0.97054\n",
      "[5000]\ttrain's auc: 0.985991\tvalid's auc: 0.970541\n",
      "[5200]\ttrain's auc: 0.98645\tvalid's auc: 0.970585\n",
      "[5400]\ttrain's auc: 0.986863\tvalid's auc: 0.9706\n",
      "[5600]\ttrain's auc: 0.987304\tvalid's auc: 0.970592\n",
      "[5800]\ttrain's auc: 0.987675\tvalid's auc: 0.97058\n",
      "[6000]\ttrain's auc: 0.988034\tvalid's auc: 0.970595\n",
      "[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016476 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3040\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n",
      "[LightGBM] [Info] Start training from score -1.986273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mykytasalykin/Desktop/kaggle-bankterm/.venv/lib/python3.12/site-packages/lightgbm/callback.py:333: UserWarning: Early stopping is not available in dart mode\n",
      "  _log_warning(\"Early stopping is not available in dart mode\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttrain's auc: 0.96219\tvalid's auc: 0.96039\n",
      "[400]\ttrain's auc: 0.964881\tvalid's auc: 0.96263\n",
      "[600]\ttrain's auc: 0.966932\tvalid's auc: 0.964156\n",
      "[800]\ttrain's auc: 0.969812\tvalid's auc: 0.966097\n",
      "[1000]\ttrain's auc: 0.971611\tvalid's auc: 0.966953\n",
      "[1200]\ttrain's auc: 0.972976\tvalid's auc: 0.967535\n",
      "[1400]\ttrain's auc: 0.97403\tvalid's auc: 0.967879\n",
      "[1600]\ttrain's auc: 0.975234\tvalid's auc: 0.968259\n",
      "[1800]\ttrain's auc: 0.976169\tvalid's auc: 0.968474\n",
      "[2000]\ttrain's auc: 0.977069\tvalid's auc: 0.968621\n",
      "[2200]\ttrain's auc: 0.977776\tvalid's auc: 0.96873\n",
      "[2400]\ttrain's auc: 0.978493\tvalid's auc: 0.968803\n",
      "[2600]\ttrain's auc: 0.97917\tvalid's auc: 0.968907\n",
      "[2800]\ttrain's auc: 0.979791\tvalid's auc: 0.968971\n",
      "[3000]\ttrain's auc: 0.980522\tvalid's auc: 0.96905\n",
      "[3200]\ttrain's auc: 0.981142\tvalid's auc: 0.969131\n",
      "[3400]\ttrain's auc: 0.981842\tvalid's auc: 0.969235\n",
      "[3600]\ttrain's auc: 0.982438\tvalid's auc: 0.969275\n",
      "[3800]\ttrain's auc: 0.983097\tvalid's auc: 0.969292\n",
      "[4000]\ttrain's auc: 0.983584\tvalid's auc: 0.969329\n",
      "[4200]\ttrain's auc: 0.984163\tvalid's auc: 0.969361\n",
      "[4400]\ttrain's auc: 0.984687\tvalid's auc: 0.969342\n",
      "[4600]\ttrain's auc: 0.985255\tvalid's auc: 0.969376\n",
      "[4800]\ttrain's auc: 0.985723\tvalid's auc: 0.969374\n",
      "[5000]\ttrain's auc: 0.986214\tvalid's auc: 0.969418\n",
      "[5200]\ttrain's auc: 0.986661\tvalid's auc: 0.969433\n",
      "[5400]\ttrain's auc: 0.987062\tvalid's auc: 0.969443\n",
      "[5600]\ttrain's auc: 0.987494\tvalid's auc: 0.969427\n",
      "[5800]\ttrain's auc: 0.987886\tvalid's auc: 0.969426\n",
      "[6000]\ttrain's auc: 0.988246\tvalid's auc: 0.969436\n",
      "[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013522 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3038\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n",
      "[LightGBM] [Info] Start training from score -1.986289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mykytasalykin/Desktop/kaggle-bankterm/.venv/lib/python3.12/site-packages/lightgbm/callback.py:333: UserWarning: Early stopping is not available in dart mode\n",
      "  _log_warning(\"Early stopping is not available in dart mode\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttrain's auc: 0.962156\tvalid's auc: 0.960231\n",
      "[400]\ttrain's auc: 0.964865\tvalid's auc: 0.9625\n",
      "[600]\ttrain's auc: 0.96696\tvalid's auc: 0.964076\n",
      "[800]\ttrain's auc: 0.969772\tvalid's auc: 0.965929\n",
      "[1000]\ttrain's auc: 0.971645\tvalid's auc: 0.96689\n",
      "[1200]\ttrain's auc: 0.972942\tvalid's auc: 0.967369\n",
      "[1400]\ttrain's auc: 0.973999\tvalid's auc: 0.967668\n",
      "[1600]\ttrain's auc: 0.975178\tvalid's auc: 0.968018\n",
      "[1800]\ttrain's auc: 0.976113\tvalid's auc: 0.968232\n",
      "[2000]\ttrain's auc: 0.977068\tvalid's auc: 0.968452\n",
      "[2200]\ttrain's auc: 0.97774\tvalid's auc: 0.968502\n",
      "[2400]\ttrain's auc: 0.97848\tvalid's auc: 0.968616\n",
      "[2600]\ttrain's auc: 0.979118\tvalid's auc: 0.96867\n",
      "[2800]\ttrain's auc: 0.979748\tvalid's auc: 0.968725\n",
      "[3000]\ttrain's auc: 0.980492\tvalid's auc: 0.968815\n",
      "[3200]\ttrain's auc: 0.981151\tvalid's auc: 0.968909\n",
      "[3400]\ttrain's auc: 0.9818\tvalid's auc: 0.968969\n",
      "[3600]\ttrain's auc: 0.98241\tvalid's auc: 0.969003\n",
      "[3800]\ttrain's auc: 0.983079\tvalid's auc: 0.969064\n",
      "[4000]\ttrain's auc: 0.983557\tvalid's auc: 0.969098\n",
      "[4200]\ttrain's auc: 0.98414\tvalid's auc: 0.969119\n",
      "[4400]\ttrain's auc: 0.984651\tvalid's auc: 0.969151\n",
      "[4600]\ttrain's auc: 0.985211\tvalid's auc: 0.969173\n",
      "[4800]\ttrain's auc: 0.985689\tvalid's auc: 0.969169\n",
      "[5000]\ttrain's auc: 0.986193\tvalid's auc: 0.969181\n",
      "[5200]\ttrain's auc: 0.986617\tvalid's auc: 0.969167\n",
      "[5400]\ttrain's auc: 0.987034\tvalid's auc: 0.969192\n",
      "[5600]\ttrain's auc: 0.987484\tvalid's auc: 0.969194\n",
      "[5800]\ttrain's auc: 0.987894\tvalid's auc: 0.969216\n",
      "[6000]\ttrain's auc: 0.98824\tvalid's auc: 0.969215\n",
      "[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011771 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3035\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n",
      "[LightGBM] [Info] Start training from score -1.986289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mykytasalykin/Desktop/kaggle-bankterm/.venv/lib/python3.12/site-packages/lightgbm/callback.py:333: UserWarning: Early stopping is not available in dart mode\n",
      "  _log_warning(\"Early stopping is not available in dart mode\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttrain's auc: 0.961908\tvalid's auc: 0.961326\n",
      "[400]\ttrain's auc: 0.964658\tvalid's auc: 0.963397\n",
      "[600]\ttrain's auc: 0.966785\tvalid's auc: 0.964954\n",
      "[800]\ttrain's auc: 0.96957\tvalid's auc: 0.966838\n",
      "[1000]\ttrain's auc: 0.971447\tvalid's auc: 0.967856\n",
      "[1200]\ttrain's auc: 0.97283\tvalid's auc: 0.968421\n",
      "[1400]\ttrain's auc: 0.973862\tvalid's auc: 0.968679\n",
      "[1600]\ttrain's auc: 0.975057\tvalid's auc: 0.969072\n",
      "[1800]\ttrain's auc: 0.975965\tvalid's auc: 0.969243\n",
      "[2000]\ttrain's auc: 0.976938\tvalid's auc: 0.96948\n",
      "[2200]\ttrain's auc: 0.977588\tvalid's auc: 0.969559\n",
      "[2400]\ttrain's auc: 0.978326\tvalid's auc: 0.969672\n",
      "[2600]\ttrain's auc: 0.978981\tvalid's auc: 0.96974\n",
      "[2800]\ttrain's auc: 0.979624\tvalid's auc: 0.969828\n",
      "[3000]\ttrain's auc: 0.980369\tvalid's auc: 0.96995\n",
      "[3200]\ttrain's auc: 0.981003\tvalid's auc: 0.970032\n",
      "[3400]\ttrain's auc: 0.981672\tvalid's auc: 0.970127\n",
      "[3600]\ttrain's auc: 0.982285\tvalid's auc: 0.970163\n",
      "[3800]\ttrain's auc: 0.98297\tvalid's auc: 0.970213\n",
      "[4000]\ttrain's auc: 0.983441\tvalid's auc: 0.970233\n",
      "[4200]\ttrain's auc: 0.984051\tvalid's auc: 0.970311\n",
      "[4400]\ttrain's auc: 0.984575\tvalid's auc: 0.970329\n",
      "[4600]\ttrain's auc: 0.985129\tvalid's auc: 0.970344\n",
      "[4800]\ttrain's auc: 0.985613\tvalid's auc: 0.970341\n",
      "[5000]\ttrain's auc: 0.98611\tvalid's auc: 0.970351\n",
      "[5200]\ttrain's auc: 0.986548\tvalid's auc: 0.970367\n",
      "[5400]\ttrain's auc: 0.986966\tvalid's auc: 0.970373\n",
      "[5600]\ttrain's auc: 0.987396\tvalid's auc: 0.970369\n",
      "[5800]\ttrain's auc: 0.987793\tvalid's auc: 0.970365\n",
      "[6000]\ttrain's auc: 0.988141\tvalid's auc: 0.97034\n",
      "[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011811 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3039\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n",
      "[LightGBM] [Info] Start training from score -1.986289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mykytasalykin/Desktop/kaggle-bankterm/.venv/lib/python3.12/site-packages/lightgbm/callback.py:333: UserWarning: Early stopping is not available in dart mode\n",
      "  _log_warning(\"Early stopping is not available in dart mode\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttrain's auc: 0.962227\tvalid's auc: 0.960676\n",
      "[400]\ttrain's auc: 0.964887\tvalid's auc: 0.962831\n",
      "[600]\ttrain's auc: 0.966899\tvalid's auc: 0.964334\n",
      "[800]\ttrain's auc: 0.969709\tvalid's auc: 0.966196\n",
      "[1000]\ttrain's auc: 0.971652\tvalid's auc: 0.967282\n",
      "[1200]\ttrain's auc: 0.972998\tvalid's auc: 0.96786\n",
      "[1400]\ttrain's auc: 0.974044\tvalid's auc: 0.968188\n",
      "[1600]\ttrain's auc: 0.975256\tvalid's auc: 0.968581\n",
      "[1800]\ttrain's auc: 0.976198\tvalid's auc: 0.96881\n",
      "[2000]\ttrain's auc: 0.977105\tvalid's auc: 0.968989\n",
      "[2200]\ttrain's auc: 0.977797\tvalid's auc: 0.969076\n",
      "[2400]\ttrain's auc: 0.978526\tvalid's auc: 0.96918\n",
      "[2600]\ttrain's auc: 0.979168\tvalid's auc: 0.969245\n",
      "[2800]\ttrain's auc: 0.979778\tvalid's auc: 0.969277\n",
      "[3000]\ttrain's auc: 0.9805\tvalid's auc: 0.969357\n",
      "[3200]\ttrain's auc: 0.98114\tvalid's auc: 0.969428\n",
      "[3400]\ttrain's auc: 0.981783\tvalid's auc: 0.969467\n",
      "[3600]\ttrain's auc: 0.982388\tvalid's auc: 0.969527\n",
      "[3800]\ttrain's auc: 0.983052\tvalid's auc: 0.969602\n",
      "[4000]\ttrain's auc: 0.983529\tvalid's auc: 0.969653\n",
      "[4200]\ttrain's auc: 0.984107\tvalid's auc: 0.969662\n",
      "[4400]\ttrain's auc: 0.984655\tvalid's auc: 0.969704\n",
      "[4600]\ttrain's auc: 0.985239\tvalid's auc: 0.969739\n",
      "[4800]\ttrain's auc: 0.985741\tvalid's auc: 0.969747\n",
      "[5000]\ttrain's auc: 0.986226\tvalid's auc: 0.969767\n",
      "[5200]\ttrain's auc: 0.986673\tvalid's auc: 0.969797\n",
      "[5400]\ttrain's auc: 0.987073\tvalid's auc: 0.969816\n",
      "[5600]\ttrain's auc: 0.987487\tvalid's auc: 0.969843\n",
      "[5800]\ttrain's auc: 0.987871\tvalid's auc: 0.969837\n",
      "[6000]\ttrain's auc: 0.98823\tvalid's auc: 0.969847\n",
      "GBDT OOF AUC: 0.9696264982320634\n",
      "DART OOF AUC: 0.9698861108411851\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from pathlib import Path\n",
    "\n",
    "pos_weight = (y==0).sum()/(y==1).sum()\n",
    "\n",
    "def cv_lgbm(X_, y_, params, n_splits=5, seed=42):\n",
    "    oof = np.zeros(len(X_))\n",
    "    test_pred = np.zeros(len(X_test_te))\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    for tr, va in skf.split(X_, y_):\n",
    "        X_tr, X_va = X_.iloc[tr], X_.iloc[va]\n",
    "        y_tr, y_va = y_[tr], y_[va]\n",
    "        dtr = lgb.Dataset(X_tr, label=y_tr, categorical_feature=cat_cols, free_raw_data=False)\n",
    "        dva = lgb.Dataset(X_va, label=y_va, categorical_feature=cat_cols, free_raw_data=False)\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            dtr,\n",
    "            valid_sets=[dtr, dva],\n",
    "            valid_names=['train','valid'],\n",
    "            num_boost_round=6000,\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=300), lgb.log_evaluation(period=200)],\n",
    "        )\n",
    "        oof[va] = model.predict(X_va, num_iteration=model.best_iteration)\n",
    "        test_pred += model.predict(X_test_te, num_iteration=model.best_iteration)/n_splits\n",
    "    return oof, test_pred\n",
    "\n",
    "params_gbdt = {\n",
    "    \"objective\":\"binary\",\n",
    "    \"metric\":\"auc\",\n",
    "    \"boosting_type\":\"gbdt\",\n",
    "    \"learning_rate\":0.03,\n",
    "    \"num_leaves\":127,\n",
    "    \"min_data_in_leaf\":64,\n",
    "    \"min_sum_hessian_in_leaf\":5.0,\n",
    "    \"feature_fraction\":0.85,\n",
    "    \"bagging_fraction\":0.85,\n",
    "    \"bagging_freq\":1,\n",
    "    \"lambda_l1\":0.0,\n",
    "    \"lambda_l2\":10.0,\n",
    "    \"scale_pos_weight\":pos_weight,\n",
    "    \"seed\":42,\n",
    "    \"n_jobs\":-1,\n",
    "}\n",
    "\n",
    "params_dart = dict(params_gbdt)\n",
    "params_dart.update({\"boosting_type\":\"dart\", \"drop_rate\":0.1, \"skip_drop\":0.5, \"max_drop\":50})\n",
    "\n",
    "oof_gbdt, test_gbdt = cv_lgbm(X_te, y, params_gbdt)\n",
    "oof_dart, test_dart = cv_lgbm(X_te, y, params_dart)\n",
    "\n",
    "print(\"GBDT OOF AUC:\", roc_auc_score(y, oof_gbdt))\n",
    "print(\"DART OOF AUC:\", roc_auc_score(y, oof_dart))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.9241523\tbest: 0.9241523 (0)\ttotal: 165ms\tremaining: 6m 52s\n",
      "200:\ttest: 0.9611855\tbest: 0.9611855 (200)\ttotal: 30.9s\tremaining: 5m 53s\n",
      "400:\ttest: 0.9638017\tbest: 0.9638017 (400)\ttotal: 1m 3s\tremaining: 5m 34s\n",
      "600:\ttest: 0.9649772\tbest: 0.9649772 (600)\ttotal: 1m 38s\tremaining: 5m 12s\n",
      "800:\ttest: 0.9657168\tbest: 0.9657168 (800)\ttotal: 2m 15s\tremaining: 4m 46s\n",
      "1000:\ttest: 0.9662553\tbest: 0.9662553 (1000)\ttotal: 2m 54s\tremaining: 4m 21s\n",
      "1200:\ttest: 0.9666308\tbest: 0.9666308 (1200)\ttotal: 3m 32s\tremaining: 3m 50s\n",
      "1400:\ttest: 0.9668863\tbest: 0.9668863 (1400)\ttotal: 4m 10s\tremaining: 3m 16s\n",
      "1600:\ttest: 0.9671883\tbest: 0.9671891 (1599)\ttotal: 4m 46s\tremaining: 2m 41s\n",
      "1800:\ttest: 0.9673948\tbest: 0.9673948 (1800)\ttotal: 5m 25s\tremaining: 2m 6s\n",
      "2000:\ttest: 0.9675745\tbest: 0.9675746 (1999)\ttotal: 6m 5s\tremaining: 1m 31s\n",
      "2200:\ttest: 0.9677352\tbest: 0.9677352 (2200)\ttotal: 6m 46s\tremaining: 55.2s\n",
      "2400:\ttest: 0.9678814\tbest: 0.9678814 (2400)\ttotal: 7m 26s\tremaining: 18.4s\n",
      "2499:\ttest: 0.9679378\tbest: 0.9679378 (2499)\ttotal: 7m 46s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9679377918\n",
      "bestIteration = 2499\n",
      "\n",
      "0:\ttest: 0.9283331\tbest: 0.9283331 (0)\ttotal: 230ms\tremaining: 9m 34s\n",
      "200:\ttest: 0.9603125\tbest: 0.9603125 (200)\ttotal: 40.5s\tremaining: 7m 42s\n",
      "400:\ttest: 0.9627873\tbest: 0.9627873 (400)\ttotal: 1m 20s\tremaining: 7m\n",
      "600:\ttest: 0.9639334\tbest: 0.9639334 (600)\ttotal: 2m\tremaining: 6m 21s\n",
      "800:\ttest: 0.9646830\tbest: 0.9646830 (800)\ttotal: 2m 41s\tremaining: 5m 41s\n",
      "1000:\ttest: 0.9652110\tbest: 0.9652114 (999)\ttotal: 3m 21s\tremaining: 5m 2s\n",
      "1200:\ttest: 0.9655866\tbest: 0.9655866 (1200)\ttotal: 4m 2s\tremaining: 4m 22s\n",
      "1400:\ttest: 0.9658721\tbest: 0.9658721 (1400)\ttotal: 4m 44s\tremaining: 3m 42s\n",
      "1600:\ttest: 0.9660860\tbest: 0.9660867 (1598)\ttotal: 5m 27s\tremaining: 3m 4s\n",
      "1800:\ttest: 0.9663085\tbest: 0.9663085 (1800)\ttotal: 6m 10s\tremaining: 2m 23s\n",
      "2000:\ttest: 0.9664963\tbest: 0.9664968 (1999)\ttotal: 6m 52s\tremaining: 1m 42s\n",
      "2200:\ttest: 0.9666933\tbest: 0.9666933 (2200)\ttotal: 7m 34s\tremaining: 1m 1s\n",
      "2400:\ttest: 0.9668115\tbest: 0.9668115 (2400)\ttotal: 8m 16s\tremaining: 20.5s\n",
      "2499:\ttest: 0.9668809\tbest: 0.9668809 (2499)\ttotal: 8m 37s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9668809486\n",
      "bestIteration = 2499\n",
      "\n",
      "0:\ttest: 0.9261719\tbest: 0.9261719 (0)\ttotal: 262ms\tremaining: 10m 54s\n",
      "200:\ttest: 0.9602628\tbest: 0.9602628 (200)\ttotal: 44.2s\tremaining: 8m 25s\n",
      "400:\ttest: 0.9628099\tbest: 0.9628099 (400)\ttotal: 1m 26s\tremaining: 7m 34s\n",
      "600:\ttest: 0.9638901\tbest: 0.9638901 (600)\ttotal: 2m 10s\tremaining: 6m 52s\n",
      "800:\ttest: 0.9646146\tbest: 0.9646146 (800)\ttotal: 2m 54s\tremaining: 6m 9s\n",
      "1000:\ttest: 0.9651511\tbest: 0.9651511 (1000)\ttotal: 3m 38s\tremaining: 5m 27s\n",
      "1200:\ttest: 0.9654979\tbest: 0.9654979 (1200)\ttotal: 4m 22s\tremaining: 4m 43s\n",
      "1400:\ttest: 0.9658088\tbest: 0.9658088 (1400)\ttotal: 5m 6s\tremaining: 4m\n",
      "1600:\ttest: 0.9660427\tbest: 0.9660429 (1598)\ttotal: 5m 49s\tremaining: 3m 16s\n",
      "1800:\ttest: 0.9662658\tbest: 0.9662661 (1799)\ttotal: 6m 34s\tremaining: 2m 33s\n",
      "2000:\ttest: 0.9664434\tbest: 0.9664434 (2000)\ttotal: 7m 19s\tremaining: 1m 49s\n",
      "2200:\ttest: 0.9665812\tbest: 0.9665812 (2200)\ttotal: 8m 5s\tremaining: 1m 5s\n",
      "2400:\ttest: 0.9667441\tbest: 0.9667449 (2399)\ttotal: 8m 50s\tremaining: 21.9s\n",
      "2499:\ttest: 0.9668072\tbest: 0.9668072 (2499)\ttotal: 9m 12s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9668071556\n",
      "bestIteration = 2499\n",
      "\n",
      "0:\ttest: 0.9273751\tbest: 0.9273751 (0)\ttotal: 256ms\tremaining: 10m 40s\n",
      "200:\ttest: 0.9612627\tbest: 0.9612627 (200)\ttotal: 44.8s\tremaining: 8m 32s\n",
      "400:\ttest: 0.9636937\tbest: 0.9636937 (400)\ttotal: 1m 28s\tremaining: 7m 42s\n",
      "600:\ttest: 0.9648039\tbest: 0.9648039 (600)\ttotal: 2m 13s\tremaining: 7m 3s\n",
      "800:\ttest: 0.9654187\tbest: 0.9654188 (799)\ttotal: 2m 57s\tremaining: 6m 16s\n",
      "1000:\ttest: 0.9658849\tbest: 0.9658849 (1000)\ttotal: 3m 40s\tremaining: 5m 30s\n",
      "1200:\ttest: 0.9662224\tbest: 0.9662224 (1200)\ttotal: 4m 24s\tremaining: 4m 45s\n",
      "1400:\ttest: 0.9665741\tbest: 0.9665741 (1400)\ttotal: 5m 8s\tremaining: 4m 1s\n",
      "1600:\ttest: 0.9668480\tbest: 0.9668480 (1600)\ttotal: 5m 52s\tremaining: 3m 18s\n",
      "1800:\ttest: 0.9670807\tbest: 0.9670807 (1800)\ttotal: 6m 37s\tremaining: 2m 34s\n",
      "2000:\ttest: 0.9672645\tbest: 0.9672647 (1999)\ttotal: 7m 22s\tremaining: 1m 50s\n",
      "2200:\ttest: 0.9674030\tbest: 0.9674030 (2200)\ttotal: 8m 6s\tremaining: 1m 6s\n",
      "2400:\ttest: 0.9675574\tbest: 0.9675574 (2400)\ttotal: 8m 51s\tremaining: 21.9s\n",
      "2499:\ttest: 0.9676310\tbest: 0.9676310 (2499)\ttotal: 9m 13s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.967631043\n",
      "bestIteration = 2499\n",
      "\n",
      "0:\ttest: 0.9269932\tbest: 0.9269932 (0)\ttotal: 226ms\tremaining: 9m 23s\n",
      "200:\ttest: 0.9606943\tbest: 0.9606943 (200)\ttotal: 44.4s\tremaining: 8m 28s\n",
      "400:\ttest: 0.9632135\tbest: 0.9632135 (400)\ttotal: 1m 27s\tremaining: 7m 39s\n",
      "600:\ttest: 0.9642829\tbest: 0.9642829 (600)\ttotal: 2m 11s\tremaining: 6m 54s\n",
      "800:\ttest: 0.9649761\tbest: 0.9649771 (799)\ttotal: 2m 55s\tremaining: 6m 11s\n",
      "1000:\ttest: 0.9654458\tbest: 0.9654458 (1000)\ttotal: 3m 39s\tremaining: 5m 28s\n",
      "1200:\ttest: 0.9658520\tbest: 0.9658524 (1199)\ttotal: 4m 24s\tremaining: 4m 45s\n",
      "1400:\ttest: 0.9661512\tbest: 0.9661512 (1400)\ttotal: 5m 8s\tremaining: 4m 2s\n",
      "1600:\ttest: 0.9663961\tbest: 0.9663961 (1600)\ttotal: 5m 52s\tremaining: 3m 18s\n",
      "1800:\ttest: 0.9665475\tbest: 0.9665475 (1800)\ttotal: 6m 37s\tremaining: 2m 34s\n",
      "2000:\ttest: 0.9667417\tbest: 0.9667419 (1999)\ttotal: 7m 22s\tremaining: 1m 50s\n",
      "2200:\ttest: 0.9668867\tbest: 0.9668867 (2200)\ttotal: 8m 7s\tremaining: 1m 6s\n",
      "2400:\ttest: 0.9670117\tbest: 0.9670117 (2400)\ttotal: 8m 52s\tremaining: 22s\n",
      "2499:\ttest: 0.9670589\tbest: 0.9670591 (2498)\ttotal: 9m 15s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9670590689\n",
      "bestIteration = 2498\n",
      "\n",
      "Shrink model to first 2499 iterations.\n",
      "Cat OOF AUC: 0.9672610540173565\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "def cv_cat(X_, y_, n_splits=5, seed=42):\n",
    "    oof = np.zeros(len(X_))\n",
    "    test_pred = np.zeros(len(X_test_te))\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    for tr, va in skf.split(X_, y_):\n",
    "        X_tr, X_va = X_.iloc[tr], X_.iloc[va]\n",
    "        y_tr, y_va = y_[tr], y_[va]\n",
    "        tr_pool = Pool(X_tr, y_tr, cat_features=cat_cols)\n",
    "        va_pool = Pool(X_va, y_va, cat_features=cat_cols)\n",
    "        te_pool = Pool(X_test_te, cat_features=cat_cols)\n",
    "        model = CatBoostClassifier(\n",
    "            iterations=2500,\n",
    "            depth=6,\n",
    "            learning_rate=0.05,\n",
    "            l2_leaf_reg=6,\n",
    "            loss_function=\"Logloss\",\n",
    "            eval_metric=\"AUC\",\n",
    "            random_seed=42,\n",
    "            od_type=\"Iter\",\n",
    "            od_wait=200,\n",
    "            verbose=200\n",
    "        )\n",
    "        model.fit(tr_pool, eval_set=va_pool, use_best_model=True)\n",
    "        oof[va] = model.predict_proba(va_pool)[:,1]\n",
    "        test_pred += model.predict_proba(te_pool)[:,1]/n_splits\n",
    "    return oof, test_pred\n",
    "\n",
    "oof_cat, test_cat = cv_cat(X_te, y)\n",
    "\n",
    "print(\"Cat OOF AUC:\", roc_auc_score(y, oof_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blend-weighted OOF AUC: 0.9696346807388398\n",
      "Blend-rank OOF AUC: 0.9697065582417187\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import rankdata\n",
    "\n",
    "def rank_avg(*cols):\n",
    "    arr = np.vstack([rankdata(c)/len(c) for c in cols])\n",
    "    return arr.mean(axis=0)\n",
    "\n",
    "w_gbdt = roc_auc_score(y, oof_gbdt)\n",
    "w_dart = roc_auc_score(y, oof_dart)\n",
    "w_cat = roc_auc_score(y, oof_cat)\n",
    "ws = np.array([w_gbdt, w_dart, w_cat])\n",
    "ws = ws/ws.sum()\n",
    "\n",
    "oof_blend = ws[0]*oof_gbdt + ws[1]*oof_dart + ws[2]*oof_cat\n",
    "oof_rank = rank_avg(oof_gbdt, oof_dart, oof_cat)\n",
    "\n",
    "print(\"Blend-weighted OOF AUC:\", roc_auc_score(y, oof_blend))\n",
    "print(\"Blend-rank OOF AUC:\", roc_auc_score(y, oof_rank))\n",
    "\n",
    "test_blend = ws[0]*test_gbdt + ws[1]*test_dart + ws[2]*test_cat\n",
    "test_rank = rank_avg(test_gbdt, test_dart, test_cat)\n",
    "\n",
    "Path(\"outputs/submissions\").mkdir(parents=True, exist_ok=True)\n",
    "pd.DataFrame({\"id\": test[\"id\"], \"y\": test_blend}).to_csv(\"outputs/submissions/blend_weighted.csv\", index=False)\n",
    "pd.DataFrame({\"id\": test[\"id\"], \"y\": test_rank}).to_csv(\"outputs/submissions/blend_rank.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "def kfold_target_encoding_smooth(train_df, test_df, cols, target, n_splits=5, seed=42, min_samples=50, smoothing=10, noise=0.01):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    te_train = pd.DataFrame(index=train_df.index)\n",
    "    te_test = pd.DataFrame(index=test_df.index)\n",
    "    global_mean = train_df[target].mean()\n",
    "    for c in cols:\n",
    "        oof_vals = np.zeros(len(train_df))\n",
    "        for tr, va in skf.split(train_df, train_df[target]):\n",
    "            tr_df, va_df = train_df.iloc[tr], train_df.iloc[va]\n",
    "            stats = tr_df.groupby(c)[target].agg(['mean','count'])\n",
    "            smooth = (stats['count']*stats['mean'] + smoothing*global_mean) / (stats['count'] + smoothing)\n",
    "            oof_vals[va] = va_df[c].map(smooth).fillna(global_mean).values\n",
    "        te_train[c+\"_te\"] = oof_vals*(1+noise*np.random.randn(len(oof_vals)))\n",
    "        stats_full = train_df.groupby(c)[target].agg(['mean','count'])\n",
    "        smooth_full = (stats_full['count']*stats_full['mean'] + smoothing*global_mean) / (stats_full['count'] + smoothing)\n",
    "        te_test[c+\"_te\"] = test_df[c].map(smooth_full).fillna(global_mean).values\n",
    "    return te_train, te_test\n",
    "\n",
    "pairs = [('job','month'),('contact','month'),('poutcome','contact')]\n",
    "for a,b in pairs:\n",
    "    train_f[f'{a}_{b}'] = train_f[a].astype(str)+'__'+train_f[b].astype(str)\n",
    "    test_f[f'{a}_{b}'] = test_f[a].astype(str)+'__'+test_f[b].astype(str)\n",
    "\n",
    "te_cols = [\"job\",\"education\",\"contact\",\"month\",\"poutcome\",\"job_month\",\"contact_month\",\"poutcome_contact\"]\n",
    "te_train2, te_test2 = kfold_target_encoding_smooth(train_f.rename(columns={'job_month':'job_month','contact_month':'contact_month','poutcome_contact':'poutcome_contact'})\\\n",
    "    .assign(job_month=train_f['job'].astype(str)+'__'+train_f['month'].astype(str),\n",
    "            contact_month=train_f['contact'].astype(str)+'__'+train_f['month'].astype(str),\n",
    "            poutcome_contact=train_f['poutcome'].astype(str)+'__'+train_f['contact'].astype(str)),\n",
    "    test_f.assign(job_month=test_f['job'].astype(str)+'__'+test_f['month'].astype(str),\n",
    "                  contact_month=test_f['contact'].astype(str)+'__'+test_f['month'].astype(str),\n",
    "                  poutcome_contact=test_f['poutcome'].astype(str)+'__'+test_f['contact'].astype(str)),\n",
    "    te_cols, target='y', n_splits=5, smoothing=20, noise=0.01)\n",
    "\n",
    "freq_cols = [\"job\",\"education\",\"contact\",\"month\",\"poutcome\"]\n",
    "for c in freq_cols:\n",
    "    vc = train_f[c].value_counts()\n",
    "    train_f[c+\"_freq\"] = train_f[c].map(vc).fillna(0).astype(int)\n",
    "    test_f[c+\"_freq\"] = test_f[c].map(vc).fillna(0).astype(int)\n",
    "\n",
    "X2 = pd.concat([X, te_train2], axis=1)\n",
    "X2_test = pd.concat([X_test, te_test2], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gbdt(X_, y_, seed, use_pos_weight=False, num_boost_round=4000):\n",
    "    params = {\n",
    "        \"objective\":\"binary\",\"metric\":\"auc\",\"boosting_type\":\"gbdt\",\n",
    "        \"learning_rate\":0.03,\"num_leaves\":127,\"min_data_in_leaf\":64,\n",
    "        \"feature_fraction\":0.85,\"bagging_fraction\":0.85,\"bagging_freq\":1,\n",
    "        \"lambda_l2\":10.0,\"seed\":seed,\"n_jobs\":-1,\"max_bin\":511\n",
    "    }\n",
    "    if use_pos_weight:\n",
    "        params[\"scale_pos_weight\"] = (y_==0).sum()/(y_==1).sum()\n",
    "    return cv_lgbm(X_, y_, params)  # без num_boost_round\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def cv_lgbm(X_, y_, params, n_splits=5, seed=42, num_boost_round=4000, es_rounds=300):\n",
    "    oof = np.zeros(len(X_))\n",
    "    test_pred = np.zeros(len(X_test_te))\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    folds = list(skf.split(X_, y_))\n",
    "    use_es = params.get(\"boosting_type\",\"gbdt\") != \"dart\"\n",
    "    for tr, va in tqdm(folds, desc=f\"LGBM-{params.get('boosting_type','gbdt')}\", leave=False):\n",
    "        X_tr, X_va = X_.iloc[tr], X_.iloc[va]\n",
    "        y_tr, y_va = y_[tr], y_[va]\n",
    "        dtr = lgb.Dataset(X_tr, label=y_tr, categorical_feature=cat_cols, free_raw_data=False)\n",
    "        dva = lgb.Dataset(X_va, label=y_va, categorical_feature=cat_cols, free_raw_data=False)\n",
    "        cbs = [lgb.log_evaluation(period=200)]\n",
    "        if use_es: cbs.append(lgb.early_stopping(stopping_rounds=es_rounds))\n",
    "        model = lgb.train(params, dtr, valid_sets=[dtr, dva], valid_names=['train','valid'],\n",
    "                          num_boost_round=num_boost_round, callbacks=cbs)\n",
    "        best_iter = model.best_iteration if use_es else num_boost_round\n",
    "        oof[va] = model.predict(X_va, num_iteration=best_iter)\n",
    "        test_pred += model.predict(X_test_te, num_iteration=best_iter)/n_splits\n",
    "    return oof, test_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def align_train_test(X_tr, X_te, cat_cols):\n",
    "    X_tr = deepcopy(X_tr)\n",
    "    X_te = deepcopy(X_te)\n",
    "    for c in cat_cols:\n",
    "        if c in X_tr.columns:\n",
    "            X_tr[c] = X_tr[c].astype(\"category\")\n",
    "        if c in X_te.columns:\n",
    "            X_te[c] = X_te[c].astype(\"category\")\n",
    "        if c in X_tr.columns and c in X_te.columns:\n",
    "            cats = sorted(set(X_tr[c].cat.categories).union(set(X_te[c].cat.categories)))\n",
    "            X_tr[c] = X_tr[c].cat.set_categories(cats)\n",
    "            X_te[c] = X_te[c].cat.set_categories(cats)\n",
    "    tr_only = [c for c in X_tr.columns if c not in X_te.columns]\n",
    "    te_only = [c for c in X_te.columns if c not in X_tr.columns]\n",
    "    for c in tr_only:\n",
    "        X_te[c] = 0\n",
    "    for c in te_only:\n",
    "        X_tr[c] = 0\n",
    "    X_tr = X_tr[X_te.columns]\n",
    "    return X_tr, X_te\n",
    "\n",
    "X2_aligned, X2_test_aligned = align_train_test(X2, X2_test, cat_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def cv_lgbm(X_tr_all, y_all, X_te_all, params, n_splits=5, seed=42, num_boost_round=4000, es_rounds=300):\n",
    "    oof = np.zeros(len(X_tr_all))\n",
    "    test_pred = np.zeros(len(X_te_all))\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    folds = list(skf.split(X_tr_all, y_all))\n",
    "    use_es = params.get(\"boosting_type\",\"gbdt\") != \"dart\"\n",
    "    for tr, va in tqdm(folds, desc=f\"LGBM-{params.get('boosting_type','gbdt')}\", leave=False):\n",
    "        X_tr, X_va = X_tr_all.iloc[tr], X_tr_all.iloc[va]\n",
    "        y_tr, y_va = y_all[tr], y_all[va]\n",
    "        dtr = lgb.Dataset(X_tr, label=y_tr, categorical_feature=cat_cols, free_raw_data=False)\n",
    "        dva = lgb.Dataset(X_va, label=y_va, categorical_feature=cat_cols, free_raw_data=False)\n",
    "        cbs = [lgb.log_evaluation(period=200)]\n",
    "        if use_es: cbs.append(lgb.early_stopping(stopping_rounds=es_rounds))\n",
    "        model = lgb.train(params, dtr, valid_sets=[dtr, dva], valid_names=['train','valid'],\n",
    "                          num_boost_round=num_boost_round, callbacks=cbs)\n",
    "        best_iter = model.best_iteration if use_es else num_boost_round\n",
    "        oof[va] = model.predict(X_va, num_iteration=best_iter)\n",
    "        test_pred += model.predict(X_te_all, num_iteration=best_iter) / n_splits\n",
    "    return oof, test_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae41a3aad6404755b5bf16d027c01c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LGBM-gbdt:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020628 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9341\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n",
      "[LightGBM] [Info] Start training from score -1.986273\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[200]\ttrain's auc: 0.970279\tvalid's auc: 0.968\n",
      "[400]\ttrain's auc: 0.974889\tvalid's auc: 0.969996\n",
      "[600]\ttrain's auc: 0.977845\tvalid's auc: 0.970491\n",
      "[800]\ttrain's auc: 0.980336\tvalid's auc: 0.970857\n",
      "[1000]\ttrain's auc: 0.982449\tvalid's auc: 0.970954\n",
      "[1200]\ttrain's auc: 0.98433\tvalid's auc: 0.971106\n",
      "[1400]\ttrain's auc: 0.986008\tvalid's auc: 0.971173\n",
      "[1600]\ttrain's auc: 0.98751\tvalid's auc: 0.971167\n",
      "Early stopping, best iteration is:\n",
      "[1423]\ttrain's auc: 0.986193\tvalid's auc: 0.971183\n",
      "[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026276 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9345\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n",
      "[LightGBM] [Info] Start training from score -1.986273\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[200]\ttrain's auc: 0.970444\tvalid's auc: 0.966736\n",
      "[400]\ttrain's auc: 0.97505\tvalid's auc: 0.968737\n",
      "[600]\ttrain's auc: 0.978041\tvalid's auc: 0.96931\n",
      "[800]\ttrain's auc: 0.980462\tvalid's auc: 0.969598\n",
      "[1000]\ttrain's auc: 0.982562\tvalid's auc: 0.969744\n",
      "[1200]\ttrain's auc: 0.984473\tvalid's auc: 0.969849\n",
      "[1400]\ttrain's auc: 0.986149\tvalid's auc: 0.969871\n",
      "[1600]\ttrain's auc: 0.987682\tvalid's auc: 0.969879\n",
      "[1800]\ttrain's auc: 0.989048\tvalid's auc: 0.969903\n",
      "[2000]\ttrain's auc: 0.990284\tvalid's auc: 0.969907\n",
      "[2200]\ttrain's auc: 0.991416\tvalid's auc: 0.969908\n",
      "Early stopping, best iteration is:\n",
      "[1939]\ttrain's auc: 0.989907\tvalid's auc: 0.96992\n",
      "[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019089 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9343\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n",
      "[LightGBM] [Info] Start training from score -1.986289\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[200]\ttrain's auc: 0.970551\tvalid's auc: 0.96685\n",
      "[400]\ttrain's auc: 0.975093\tvalid's auc: 0.968839\n",
      "[600]\ttrain's auc: 0.97802\tvalid's auc: 0.969359\n",
      "[800]\ttrain's auc: 0.980451\tvalid's auc: 0.969642\n",
      "[1000]\ttrain's auc: 0.982586\tvalid's auc: 0.969833\n",
      "[1200]\ttrain's auc: 0.984481\tvalid's auc: 0.969945\n",
      "[1400]\ttrain's auc: 0.986181\tvalid's auc: 0.969989\n",
      "[1600]\ttrain's auc: 0.987694\tvalid's auc: 0.970004\n",
      "[1800]\ttrain's auc: 0.989055\tvalid's auc: 0.969983\n",
      "Early stopping, best iteration is:\n",
      "[1619]\ttrain's auc: 0.987828\tvalid's auc: 0.970007\n",
      "[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018765 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9337\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n",
      "[LightGBM] [Info] Start training from score -1.986289\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[200]\ttrain's auc: 0.970285\tvalid's auc: 0.967785\n",
      "[400]\ttrain's auc: 0.974894\tvalid's auc: 0.969652\n",
      "[600]\ttrain's auc: 0.977857\tvalid's auc: 0.970149\n",
      "[800]\ttrain's auc: 0.980307\tvalid's auc: 0.97044\n",
      "[1000]\ttrain's auc: 0.982429\tvalid's auc: 0.970617\n",
      "[1200]\ttrain's auc: 0.984335\tvalid's auc: 0.970655\n",
      "[1400]\ttrain's auc: 0.986004\tvalid's auc: 0.970698\n",
      "[1600]\ttrain's auc: 0.987554\tvalid's auc: 0.970693\n",
      "[1800]\ttrain's auc: 0.988939\tvalid's auc: 0.970695\n",
      "Early stopping, best iteration is:\n",
      "[1664]\ttrain's auc: 0.988029\tvalid's auc: 0.970712\n",
      "[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022927 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9350\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n",
      "[LightGBM] [Info] Start training from score -1.986289\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[200]\ttrain's auc: 0.970383\tvalid's auc: 0.96705\n",
      "[400]\ttrain's auc: 0.975043\tvalid's auc: 0.969015\n",
      "[600]\ttrain's auc: 0.977939\tvalid's auc: 0.969495\n",
      "[800]\ttrain's auc: 0.980377\tvalid's auc: 0.96972\n",
      "[1000]\ttrain's auc: 0.982507\tvalid's auc: 0.969857\n",
      "[1200]\ttrain's auc: 0.984358\tvalid's auc: 0.969903\n",
      "[1400]\ttrain's auc: 0.986064\tvalid's auc: 0.969942\n",
      "[1600]\ttrain's auc: 0.987609\tvalid's auc: 0.969961\n",
      "[1800]\ttrain's auc: 0.98896\tvalid's auc: 0.969949\n",
      "Early stopping, best iteration is:\n",
      "[1526]\ttrain's auc: 0.987062\tvalid's auc: 0.969972\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3de02459b4e04dc8a9694d13011c629e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LGBM-gbdt:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019266 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9338\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n",
      "[LightGBM] [Info] Start training from score -1.986273\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[200]\ttrain's auc: 0.97026\tvalid's auc: 0.967931\n",
      "[400]\ttrain's auc: 0.974852\tvalid's auc: 0.969904\n",
      "[600]\ttrain's auc: 0.97785\tvalid's auc: 0.970438\n",
      "[800]\ttrain's auc: 0.980299\tvalid's auc: 0.970732\n",
      "[1000]\ttrain's auc: 0.982448\tvalid's auc: 0.970898\n",
      "[1200]\ttrain's auc: 0.984323\tvalid's auc: 0.970995\n",
      "[1400]\ttrain's auc: 0.985995\tvalid's auc: 0.971031\n",
      "[1600]\ttrain's auc: 0.987509\tvalid's auc: 0.971063\n",
      "[1800]\ttrain's auc: 0.988879\tvalid's auc: 0.971074\n",
      "[2000]\ttrain's auc: 0.990121\tvalid's auc: 0.971077\n",
      "[2200]\ttrain's auc: 0.99125\tvalid's auc: 0.971087\n",
      "[2400]\ttrain's auc: 0.99227\tvalid's auc: 0.971052\n",
      "Early stopping, best iteration is:\n",
      "[2181]\ttrain's auc: 0.991153\tvalid's auc: 0.971099\n",
      "[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018390 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9350\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n",
      "[LightGBM] [Info] Start training from score -1.986273\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[200]\ttrain's auc: 0.970563\tvalid's auc: 0.966872\n",
      "[400]\ttrain's auc: 0.975101\tvalid's auc: 0.968703\n",
      "[600]\ttrain's auc: 0.978093\tvalid's auc: 0.969265\n",
      "[800]\ttrain's auc: 0.980565\tvalid's auc: 0.969566\n",
      "[1000]\ttrain's auc: 0.982673\tvalid's auc: 0.969697\n",
      "[1200]\ttrain's auc: 0.984514\tvalid's auc: 0.96978\n",
      "[1400]\ttrain's auc: 0.986187\tvalid's auc: 0.969837\n",
      "[1600]\ttrain's auc: 0.987679\tvalid's auc: 0.969877\n",
      "[1800]\ttrain's auc: 0.98904\tvalid's auc: 0.969903\n",
      "[2000]\ttrain's auc: 0.990275\tvalid's auc: 0.969904\n",
      "Early stopping, best iteration is:\n",
      "[1818]\ttrain's auc: 0.98915\tvalid's auc: 0.969916\n",
      "[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022940 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9341\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n",
      "[LightGBM] [Info] Start training from score -1.986289\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[200]\ttrain's auc: 0.970473\tvalid's auc: 0.966803\n",
      "[400]\ttrain's auc: 0.975135\tvalid's auc: 0.968872\n",
      "[600]\ttrain's auc: 0.978066\tvalid's auc: 0.969462\n",
      "[800]\ttrain's auc: 0.980493\tvalid's auc: 0.9697\n",
      "[1000]\ttrain's auc: 0.982555\tvalid's auc: 0.969849\n",
      "[1200]\ttrain's auc: 0.984443\tvalid's auc: 0.969925\n",
      "[1400]\ttrain's auc: 0.986119\tvalid's auc: 0.969977\n",
      "[1600]\ttrain's auc: 0.987634\tvalid's auc: 0.970007\n",
      "[1800]\ttrain's auc: 0.989025\tvalid's auc: 0.970022\n",
      "[2000]\ttrain's auc: 0.99028\tvalid's auc: 0.970009\n",
      "Early stopping, best iteration is:\n",
      "[1884]\ttrain's auc: 0.989564\tvalid's auc: 0.970027\n",
      "[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019571 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9346\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n",
      "[LightGBM] [Info] Start training from score -1.986289\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[200]\ttrain's auc: 0.970316\tvalid's auc: 0.967744\n",
      "[400]\ttrain's auc: 0.974935\tvalid's auc: 0.969627\n",
      "[600]\ttrain's auc: 0.977944\tvalid's auc: 0.97019\n",
      "[800]\ttrain's auc: 0.980396\tvalid's auc: 0.970433\n",
      "[1000]\ttrain's auc: 0.982547\tvalid's auc: 0.970598\n",
      "[1200]\ttrain's auc: 0.984407\tvalid's auc: 0.970665\n",
      "[1400]\ttrain's auc: 0.986063\tvalid's auc: 0.970691\n",
      "[1600]\ttrain's auc: 0.987595\tvalid's auc: 0.970708\n",
      "[1800]\ttrain's auc: 0.988958\tvalid's auc: 0.970716\n",
      "Early stopping, best iteration is:\n",
      "[1673]\ttrain's auc: 0.988105\tvalid's auc: 0.970722\n",
      "[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018420 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9333\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n",
      "[LightGBM] [Info] Start training from score -1.986289\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[200]\ttrain's auc: 0.970446\tvalid's auc: 0.967151\n",
      "[400]\ttrain's auc: 0.975066\tvalid's auc: 0.969084\n",
      "[600]\ttrain's auc: 0.978043\tvalid's auc: 0.969601\n",
      "[800]\ttrain's auc: 0.980512\tvalid's auc: 0.969885\n",
      "[1000]\ttrain's auc: 0.982637\tvalid's auc: 0.970017\n",
      "[1200]\ttrain's auc: 0.984493\tvalid's auc: 0.970082\n",
      "[1400]\ttrain's auc: 0.986161\tvalid's auc: 0.970098\n",
      "[1600]\ttrain's auc: 0.987684\tvalid's auc: 0.97012\n",
      "[1800]\ttrain's auc: 0.989043\tvalid's auc: 0.970121\n",
      "Early stopping, best iteration is:\n",
      "[1581]\ttrain's auc: 0.987545\tvalid's auc: 0.970129\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c020b3b2f2c44027a9a9d2c045cc1ddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LGBM-gbdt:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071896 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9383\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n",
      "[LightGBM] [Info] Start training from score -1.986273\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[200]\ttrain's auc: 0.96991\tvalid's auc: 0.967132\n",
      "[400]\ttrain's auc: 0.974949\tvalid's auc: 0.969227\n",
      "[600]\ttrain's auc: 0.978147\tvalid's auc: 0.969785\n",
      "[800]\ttrain's auc: 0.980781\tvalid's auc: 0.970116\n",
      "[1000]\ttrain's auc: 0.98299\tvalid's auc: 0.970298\n",
      "[1200]\ttrain's auc: 0.984994\tvalid's auc: 0.970442\n",
      "[1400]\ttrain's auc: 0.986737\tvalid's auc: 0.970547\n",
      "[1600]\ttrain's auc: 0.98827\tvalid's auc: 0.970613\n",
      "[1800]\ttrain's auc: 0.989634\tvalid's auc: 0.970632\n",
      "[2000]\ttrain's auc: 0.99085\tvalid's auc: 0.970666\n",
      "[2200]\ttrain's auc: 0.991953\tvalid's auc: 0.970669\n",
      "[2400]\ttrain's auc: 0.99295\tvalid's auc: 0.970669\n",
      "[2600]\ttrain's auc: 0.993828\tvalid's auc: 0.970668\n",
      "Early stopping, best iteration is:\n",
      "[2466]\ttrain's auc: 0.993254\tvalid's auc: 0.97069\n",
      "[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9336\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n",
      "[LightGBM] [Info] Start training from score -1.986273\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[200]\ttrain's auc: 0.970164\tvalid's auc: 0.966036\n",
      "[400]\ttrain's auc: 0.97519\tvalid's auc: 0.968095\n",
      "[600]\ttrain's auc: 0.978411\tvalid's auc: 0.968741\n",
      "[800]\ttrain's auc: 0.98107\tvalid's auc: 0.969117\n",
      "[1000]\ttrain's auc: 0.983236\tvalid's auc: 0.96928\n",
      "[1200]\ttrain's auc: 0.985227\tvalid's auc: 0.969443\n",
      "[1400]\ttrain's auc: 0.986954\tvalid's auc: 0.969561\n",
      "[1600]\ttrain's auc: 0.988481\tvalid's auc: 0.969597\n",
      "[1800]\ttrain's auc: 0.989862\tvalid's auc: 0.969634\n",
      "[2000]\ttrain's auc: 0.991049\tvalid's auc: 0.969642\n",
      "[2200]\ttrain's auc: 0.992124\tvalid's auc: 0.969689\n",
      "[2400]\ttrain's auc: 0.993073\tvalid's auc: 0.969696\n",
      "[2600]\ttrain's auc: 0.99394\tvalid's auc: 0.969717\n",
      "[2800]\ttrain's auc: 0.994711\tvalid's auc: 0.969706\n",
      "Early stopping, best iteration is:\n",
      "[2516]\ttrain's auc: 0.993595\tvalid's auc: 0.969722\n",
      "[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.091114 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9338\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n",
      "[LightGBM] [Info] Start training from score -1.986289\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[200]\ttrain's auc: 0.970163\tvalid's auc: 0.966041\n",
      "[400]\ttrain's auc: 0.975362\tvalid's auc: 0.968322\n",
      "[600]\ttrain's auc: 0.978509\tvalid's auc: 0.968869\n",
      "[800]\ttrain's auc: 0.981146\tvalid's auc: 0.969275\n",
      "[1000]\ttrain's auc: 0.983341\tvalid's auc: 0.969397\n",
      "[1200]\ttrain's auc: 0.98531\tvalid's auc: 0.96956\n",
      "[1400]\ttrain's auc: 0.987034\tvalid's auc: 0.969675\n",
      "[1600]\ttrain's auc: 0.988503\tvalid's auc: 0.969719\n",
      "[1800]\ttrain's auc: 0.989866\tvalid's auc: 0.969772\n",
      "[2000]\ttrain's auc: 0.991072\tvalid's auc: 0.969808\n",
      "[2200]\ttrain's auc: 0.992108\tvalid's auc: 0.969829\n",
      "[2400]\ttrain's auc: 0.993097\tvalid's auc: 0.969843\n",
      "[2600]\ttrain's auc: 0.993958\tvalid's auc: 0.969851\n",
      "[2800]\ttrain's auc: 0.994731\tvalid's auc: 0.969835\n",
      "Early stopping, best iteration is:\n",
      "[2531]\ttrain's auc: 0.993694\tvalid's auc: 0.969868\n",
      "[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.081479 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9339\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n",
      "[LightGBM] [Info] Start training from score -1.986289\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[200]\ttrain's auc: 0.969933\tvalid's auc: 0.966974\n",
      "[400]\ttrain's auc: 0.975122\tvalid's auc: 0.969173\n",
      "[600]\ttrain's auc: 0.978338\tvalid's auc: 0.969733\n",
      "[800]\ttrain's auc: 0.980925\tvalid's auc: 0.96998\n",
      "[1000]\ttrain's auc: 0.983182\tvalid's auc: 0.97017\n",
      "[1200]\ttrain's auc: 0.98516\tvalid's auc: 0.970305\n",
      "[1400]\ttrain's auc: 0.98687\tvalid's auc: 0.970337\n",
      "[1600]\ttrain's auc: 0.988412\tvalid's auc: 0.970375\n",
      "[1800]\ttrain's auc: 0.989757\tvalid's auc: 0.970416\n",
      "[2000]\ttrain's auc: 0.990979\tvalid's auc: 0.970421\n",
      "[2200]\ttrain's auc: 0.992061\tvalid's auc: 0.970434\n",
      "[2400]\ttrain's auc: 0.992987\tvalid's auc: 0.97041\n",
      "Early stopping, best iteration is:\n",
      "[2193]\ttrain's auc: 0.992033\tvalid's auc: 0.970442\n",
      "[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025345 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9338\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n",
      "[LightGBM] [Info] Start training from score -1.986289\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[200]\ttrain's auc: 0.970101\tvalid's auc: 0.966293\n",
      "[400]\ttrain's auc: 0.975214\tvalid's auc: 0.968508\n",
      "[600]\ttrain's auc: 0.978411\tvalid's auc: 0.969055\n",
      "[800]\ttrain's auc: 0.980977\tvalid's auc: 0.969291\n",
      "[1000]\ttrain's auc: 0.98327\tvalid's auc: 0.969463\n",
      "[1200]\ttrain's auc: 0.985212\tvalid's auc: 0.96956\n",
      "[1400]\ttrain's auc: 0.986899\tvalid's auc: 0.969617\n",
      "[1600]\ttrain's auc: 0.988413\tvalid's auc: 0.969652\n",
      "[1800]\ttrain's auc: 0.989755\tvalid's auc: 0.969678\n",
      "[2000]\ttrain's auc: 0.990967\tvalid's auc: 0.96968\n",
      "[2200]\ttrain's auc: 0.992059\tvalid's auc: 0.969699\n",
      "[2400]\ttrain's auc: 0.993018\tvalid's auc: 0.969696\n",
      "[2600]\ttrain's auc: 0.993868\tvalid's auc: 0.969692\n",
      "Early stopping, best iteration is:\n",
      "[2311]\ttrain's auc: 0.992617\tvalid's auc: 0.969707\n",
      "GBDT seeds: 0.9703511775381617 0.9703763609552994 0.970079112266868\n"
     ]
    }
   ],
   "source": [
    "pos_weight = (y==0).sum()/(y==1).sum()\n",
    "\n",
    "def make_params(seed, use_pos_weight=False):\n",
    "    p = {\n",
    "        \"objective\":\"binary\",\"metric\":\"auc\",\"boosting_type\":\"gbdt\",\n",
    "        \"learning_rate\":0.03,\"num_leaves\":127,\"min_data_in_leaf\":64,\n",
    "        \"min_sum_hessian_in_leaf\":5.0,\"feature_fraction\":0.85,\n",
    "        \"bagging_fraction\":0.85,\"bagging_freq\":1,\"lambda_l2\":10.0,\n",
    "        \"max_bin\":511,\"seed\":seed,\"n_jobs\":-1\n",
    "    }\n",
    "    if use_pos_weight: p[\"scale_pos_weight\"] = pos_weight\n",
    "    return p\n",
    "\n",
    "oof_g1, test_g1 = cv_lgbm(X2_aligned, y, X2_test_aligned, make_params(42, False), num_boost_round=3500)\n",
    "oof_g2, test_g2 = cv_lgbm(X2_aligned, y, X2_test_aligned, make_params(7,  False), num_boost_round=3500)\n",
    "oof_g3, test_g3 = cv_lgbm(X2_aligned, y, X2_test_aligned, make_params(2025, True), num_boost_round=3500)\n",
    "print(\"GBDT seeds:\", roc_auc_score(y,oof_g1), roc_auc_score(y,oof_g2), roc_auc_score(y,oof_g3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17367512771e4138b6f9ed32808f7d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stack-LogReg:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack OOF AUC: 0.9706332700185369\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "Z_tr = np.vstack([oof_g1, oof_g2, oof_g3, oof_cat, oof_dart]).T\n",
    "Z_te = np.vstack([test_g1, test_g2, test_g3, test_cat, test_dart]).T\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_meta = np.zeros(len(y)); pred_meta = np.zeros(len(test))\n",
    "for tr, va in tqdm(list(skf.split(Z_tr, y)), desc=\"Stack-LogReg\", leave=False):\n",
    "    m = LogisticRegression(max_iter=1000)\n",
    "    m.fit(Z_tr[tr], y[tr])\n",
    "    oof_meta[va] = m.predict_proba(Z_tr[va])[:,1]\n",
    "    pred_meta += m.predict_proba(Z_te)[:,1]/skf.n_splits\n",
    "\n",
    "print(\"Stack OOF AUC:\", roc_auc_score(y, oof_meta))\n",
    "pd.DataFrame({\"id\": test[\"id\"], \"y\": pred_meta}).to_csv(\"outputs/submissions/stack_logreg.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = list(X2_aligned.columns)\n",
    "pd.Series(feature_list).to_json(\"outputs/feature_list.json\", orient=\"values\")\n",
    "X2_aligned = X2_aligned[feature_list]\n",
    "X2_test_aligned = X2_test_aligned[feature_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "\n",
    "def add_bins_interactions(df):\n",
    "    out = df.copy()\n",
    "    out['duration_q50'] = pd.qcut(out['duration'], 50, labels=False, duplicates='drop')\n",
    "    out['balance_q20'] = pd.qcut(out['balance'].clip(lower=out['balance'].quantile(0.001), upper=out['balance'].quantile(0.999)), 20, labels=False, duplicates='drop')\n",
    "    out['age_q20'] = pd.qcut(out['age'], 20, labels=False, duplicates='drop')\n",
    "    out['campaign_bin'] = pd.cut(out['campaign'], [0,1,2,3,5,10,100], right=False, labels=False)\n",
    "    out['pdays_is_miss'] = (out['pdays']==-1).astype(int)\n",
    "    out['pdays_pos_log'] = np.log1p(out['pdays'].clip(lower=0))\n",
    "    out['dur_x_cell'] = np.log1p(out['duration']) * (out['contact']=='cellular').astype(int)\n",
    "    out['dur_x_ms'] = np.log1p(out['duration']) * out['month_sin']\n",
    "    out['dur_x_mc'] = np.log1p(out['duration']) * out['month_cos']\n",
    "    out['pdaysmiss_x_poutsucc'] = out['pdays_is_miss'] * (out['poutcome']=='success').astype(int)\n",
    "    return out\n",
    "\n",
    "train_f = add_bins_interactions(train_f)\n",
    "test_f = add_bins_interactions(test_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [('job','contact'),('job','month'),('education','contact'),('poutcome','contact')]\n",
    "for a,b in pairs:\n",
    "    train_f[f'{a}_{b}'] = train_f[a].astype(str)+'__'+train_f[b].astype(str)\n",
    "    test_f[f'{a}_{b}'] = test_f[a].astype(str)+'__'+test_f[b].astype(str)\n",
    "\n",
    "for c in ['job','education','contact','month','poutcome'] + [f'{a}_{b}' for a,b in pairs]:\n",
    "    vc = train_f[c].value_counts()\n",
    "    train_f[c+'_freq'] = train_f[c].map(vc).fillna(0).astype(int)\n",
    "    test_f[c+'_freq'] = test_f[c].map(vc).fillna(0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "def loo_te(train_df, test_df, cols, target, noise=0.01):\n",
    "    te_tr = pd.DataFrame(index=train_df.index); te_te = pd.DataFrame(index=test_df.index)\n",
    "    gmean = train_df[target].mean()\n",
    "    for c in cols:\n",
    "        grp = train_df.groupby(c)[target]\n",
    "        sum_y = grp.transform('sum')\n",
    "        cnt = grp.transform('count')\n",
    "        val = (sum_y - train_df[target]) / (cnt - 1).replace(0, np.nan)\n",
    "        te_tr[c+'_loo'] = val.fillna(gmean) * (1 + noise*np.random.randn(len(val)))\n",
    "        te_te[c+'_loo'] = test_df[c].map(grp.mean()).fillna(gmean)\n",
    "    return te_tr, te_te\n",
    "\n",
    "loo_cols = ['job','education','contact','month','poutcome'] + [f'{a}_{b}' for a,b in pairs]\n",
    "te_tr2, te_te2 = loo_te(train_f, test_f, loo_cols, 'y', noise=0.01)\n",
    "\n",
    "X3 = pd.concat([train_f.drop(columns=['y','id']), te_tr2], axis=1)\n",
    "X3_test = pd.concat([test_f.drop(columns=['id']), te_te2], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a950d7b8dcf34fe3b84db176bf4309ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LGBM-gbdt:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7265\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n",
      "[LightGBM] [Info] Start training from score -1.986273\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[200]\ttrain's auc: 0.962973\tvalid's auc: 0.96211\n",
      "[400]\ttrain's auc: 0.96683\tvalid's auc: 0.964347\n",
      "[600]\ttrain's auc: 0.968922\tvalid's auc: 0.965146\n",
      "[800]\ttrain's auc: 0.970386\tvalid's auc: 0.965505\n",
      "[1000]\ttrain's auc: 0.97154\tvalid's auc: 0.965694\n",
      "[1200]\ttrain's auc: 0.972494\tvalid's auc: 0.965824\n",
      "[1400]\ttrain's auc: 0.973317\tvalid's auc: 0.965895\n",
      "[1600]\ttrain's auc: 0.974088\tvalid's auc: 0.965964\n",
      "[1800]\ttrain's auc: 0.97478\tvalid's auc: 0.966006\n",
      "[2000]\ttrain's auc: 0.975423\tvalid's auc: 0.966033\n",
      "[2200]\ttrain's auc: 0.975993\tvalid's auc: 0.96604\n",
      "[2400]\ttrain's auc: 0.976536\tvalid's auc: 0.966048\n",
      "[2600]\ttrain's auc: 0.977073\tvalid's auc: 0.966045\n",
      "Early stopping, best iteration is:\n",
      "[2487]\ttrain's auc: 0.976775\tvalid's auc: 0.966051\n",
      "[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022181 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7271\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n",
      "[LightGBM] [Info] Start training from score -1.986273\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[200]\ttrain's auc: 0.963383\tvalid's auc: 0.961075\n",
      "[400]\ttrain's auc: 0.967074\tvalid's auc: 0.963191\n",
      "[600]\ttrain's auc: 0.969204\tvalid's auc: 0.964006\n",
      "[800]\ttrain's auc: 0.970703\tvalid's auc: 0.964373\n",
      "[1000]\ttrain's auc: 0.971839\tvalid's auc: 0.964556\n",
      "[1200]\ttrain's auc: 0.972794\tvalid's auc: 0.964632\n",
      "[1400]\ttrain's auc: 0.973629\tvalid's auc: 0.964688\n",
      "[1600]\ttrain's auc: 0.974369\tvalid's auc: 0.964705\n",
      "[1800]\ttrain's auc: 0.975046\tvalid's auc: 0.964722\n",
      "[2000]\ttrain's auc: 0.975666\tvalid's auc: 0.964745\n",
      "[2200]\ttrain's auc: 0.976242\tvalid's auc: 0.964743\n",
      "Early stopping, best iteration is:\n",
      "[2085]\ttrain's auc: 0.975915\tvalid's auc: 0.964751\n",
      "[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020718 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7270\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n",
      "[LightGBM] [Info] Start training from score -1.986289\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[200]\ttrain's auc: 0.963177\tvalid's auc: 0.960855\n",
      "[400]\ttrain's auc: 0.967001\tvalid's auc: 0.963325\n",
      "[600]\ttrain's auc: 0.969147\tvalid's auc: 0.964241\n",
      "[800]\ttrain's auc: 0.970631\tvalid's auc: 0.964604\n",
      "[1000]\ttrain's auc: 0.971805\tvalid's auc: 0.964798\n",
      "[1200]\ttrain's auc: 0.972772\tvalid's auc: 0.9649\n",
      "[1400]\ttrain's auc: 0.973614\tvalid's auc: 0.96496\n",
      "[1600]\ttrain's auc: 0.974365\tvalid's auc: 0.96498\n",
      "[1800]\ttrain's auc: 0.975046\tvalid's auc: 0.964992\n",
      "[2000]\ttrain's auc: 0.975697\tvalid's auc: 0.965001\n",
      "[2200]\ttrain's auc: 0.976279\tvalid's auc: 0.964979\n",
      "Early stopping, best iteration is:\n",
      "[1945]\ttrain's auc: 0.975529\tvalid's auc: 0.965007\n",
      "[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018446 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7264\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n",
      "[LightGBM] [Info] Start training from score -1.986289\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[200]\ttrain's auc: 0.963025\tvalid's auc: 0.961808\n",
      "[400]\ttrain's auc: 0.966774\tvalid's auc: 0.963992\n",
      "[600]\ttrain's auc: 0.968927\tvalid's auc: 0.964863\n",
      "[800]\ttrain's auc: 0.970413\tvalid's auc: 0.965288\n",
      "[1000]\ttrain's auc: 0.971579\tvalid's auc: 0.965526\n",
      "[1200]\ttrain's auc: 0.972537\tvalid's auc: 0.96567\n",
      "[1400]\ttrain's auc: 0.973388\tvalid's auc: 0.965778\n",
      "[1600]\ttrain's auc: 0.974149\tvalid's auc: 0.96582\n",
      "[1800]\ttrain's auc: 0.974825\tvalid's auc: 0.965855\n",
      "[2000]\ttrain's auc: 0.975468\tvalid's auc: 0.965886\n",
      "[2200]\ttrain's auc: 0.976029\tvalid's auc: 0.965887\n",
      "[2400]\ttrain's auc: 0.976575\tvalid's auc: 0.965886\n",
      "Early stopping, best iteration is:\n",
      "[2296]\ttrain's auc: 0.976301\tvalid's auc: 0.9659\n",
      "[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021927 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7281\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n",
      "[LightGBM] [Info] Start training from score -1.986289\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[200]\ttrain's auc: 0.963196\tvalid's auc: 0.961225\n",
      "[400]\ttrain's auc: 0.966893\tvalid's auc: 0.96352\n",
      "[600]\ttrain's auc: 0.9691\tvalid's auc: 0.964423\n",
      "[800]\ttrain's auc: 0.970583\tvalid's auc: 0.964811\n",
      "[1000]\ttrain's auc: 0.971728\tvalid's auc: 0.965015\n",
      "[1200]\ttrain's auc: 0.97269\tvalid's auc: 0.96513\n",
      "[1400]\ttrain's auc: 0.973536\tvalid's auc: 0.965198\n",
      "[1600]\ttrain's auc: 0.974276\tvalid's auc: 0.965217\n",
      "[1800]\ttrain's auc: 0.974958\tvalid's auc: 0.965226\n",
      "[2000]\ttrain's auc: 0.975579\tvalid's auc: 0.96524\n",
      "[2200]\ttrain's auc: 0.97614\tvalid's auc: 0.965236\n",
      "[2400]\ttrain's auc: 0.976676\tvalid's auc: 0.965219\n",
      "Early stopping, best iteration is:\n",
      "[2137]\ttrain's auc: 0.975971\tvalid's auc: 0.965245\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd491034a1a4c63b98ca22c983b3144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LGBM-gbdt:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022140 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7262\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n",
      "[LightGBM] [Info] Start training from score -1.986273\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[200]\ttrain's auc: 0.962937\tvalid's auc: 0.962104\n",
      "[400]\ttrain's auc: 0.966709\tvalid's auc: 0.964338\n",
      "[600]\ttrain's auc: 0.968897\tvalid's auc: 0.965187\n",
      "[800]\ttrain's auc: 0.970378\tvalid's auc: 0.965554\n",
      "[1000]\ttrain's auc: 0.971523\tvalid's auc: 0.965718\n",
      "[1200]\ttrain's auc: 0.972485\tvalid's auc: 0.965847\n",
      "[1400]\ttrain's auc: 0.97332\tvalid's auc: 0.965922\n",
      "[1600]\ttrain's auc: 0.974083\tvalid's auc: 0.965986\n",
      "[1800]\ttrain's auc: 0.974771\tvalid's auc: 0.966025\n",
      "[2000]\ttrain's auc: 0.975398\tvalid's auc: 0.966034\n",
      "[2200]\ttrain's auc: 0.975973\tvalid's auc: 0.966036\n",
      "[2400]\ttrain's auc: 0.976515\tvalid's auc: 0.966038\n",
      "Early stopping, best iteration is:\n",
      "[2161]\ttrain's auc: 0.975862\tvalid's auc: 0.966047\n",
      "[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022061 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7282\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n",
      "[LightGBM] [Info] Start training from score -1.986273\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[200]\ttrain's auc: 0.963237\tvalid's auc: 0.960899\n",
      "[400]\ttrain's auc: 0.967039\tvalid's auc: 0.963108\n",
      "[600]\ttrain's auc: 0.969169\tvalid's auc: 0.963916\n",
      "[800]\ttrain's auc: 0.970621\tvalid's auc: 0.964229\n",
      "[1000]\ttrain's auc: 0.971766\tvalid's auc: 0.964404\n",
      "[1200]\ttrain's auc: 0.972733\tvalid's auc: 0.964505\n",
      "[1400]\ttrain's auc: 0.97356\tvalid's auc: 0.964579\n",
      "[1600]\ttrain's auc: 0.974294\tvalid's auc: 0.964621\n",
      "[1800]\ttrain's auc: 0.97499\tvalid's auc: 0.964652\n",
      "[2000]\ttrain's auc: 0.97562\tvalid's auc: 0.964665\n",
      "[2200]\ttrain's auc: 0.976189\tvalid's auc: 0.964673\n",
      "[2400]\ttrain's auc: 0.976749\tvalid's auc: 0.964673\n",
      "Early stopping, best iteration is:\n",
      "[2145]\ttrain's auc: 0.976036\tvalid's auc: 0.96468\n",
      "[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023227 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7270\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n",
      "[LightGBM] [Info] Start training from score -1.986289\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[200]\ttrain's auc: 0.963216\tvalid's auc: 0.960995\n",
      "[400]\ttrain's auc: 0.966924\tvalid's auc: 0.963365\n",
      "[600]\ttrain's auc: 0.969028\tvalid's auc: 0.964187\n",
      "[800]\ttrain's auc: 0.970493\tvalid's auc: 0.964558\n",
      "[1000]\ttrain's auc: 0.971659\tvalid's auc: 0.964783\n",
      "[1200]\ttrain's auc: 0.972642\tvalid's auc: 0.964904\n",
      "[1400]\ttrain's auc: 0.973501\tvalid's auc: 0.964964\n",
      "[1600]\ttrain's auc: 0.974259\tvalid's auc: 0.964997\n",
      "[1800]\ttrain's auc: 0.974937\tvalid's auc: 0.965016\n",
      "[2000]\ttrain's auc: 0.97557\tvalid's auc: 0.965006\n",
      "Early stopping, best iteration is:\n",
      "[1802]\ttrain's auc: 0.974944\tvalid's auc: 0.965017\n",
      "[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020640 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7275\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n",
      "[LightGBM] [Info] Start training from score -1.986289\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[200]\ttrain's auc: 0.963054\tvalid's auc: 0.961864\n",
      "[400]\ttrain's auc: 0.966757\tvalid's auc: 0.964031\n",
      "[600]\ttrain's auc: 0.968942\tvalid's auc: 0.964934\n",
      "[800]\ttrain's auc: 0.970392\tvalid's auc: 0.965329\n",
      "[1000]\ttrain's auc: 0.971536\tvalid's auc: 0.965548\n",
      "[1200]\ttrain's auc: 0.972511\tvalid's auc: 0.965685\n",
      "[1400]\ttrain's auc: 0.973358\tvalid's auc: 0.965774\n",
      "[1600]\ttrain's auc: 0.974099\tvalid's auc: 0.965819\n",
      "[1800]\ttrain's auc: 0.974776\tvalid's auc: 0.965847\n",
      "[2000]\ttrain's auc: 0.97542\tvalid's auc: 0.965863\n",
      "[2200]\ttrain's auc: 0.976012\tvalid's auc: 0.965883\n",
      "[2400]\ttrain's auc: 0.976561\tvalid's auc: 0.965895\n",
      "[2600]\ttrain's auc: 0.977079\tvalid's auc: 0.965889\n",
      "Early stopping, best iteration is:\n",
      "[2376]\ttrain's auc: 0.976496\tvalid's auc: 0.9659\n",
      "[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019691 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7256\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n",
      "[LightGBM] [Info] Start training from score -1.986289\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[200]\ttrain's auc: 0.963045\tvalid's auc: 0.961199\n",
      "[400]\ttrain's auc: 0.966823\tvalid's auc: 0.963539\n",
      "[600]\ttrain's auc: 0.968994\tvalid's auc: 0.96442\n",
      "[800]\ttrain's auc: 0.970474\tvalid's auc: 0.96476\n",
      "[1000]\ttrain's auc: 0.97164\tvalid's auc: 0.964954\n",
      "[1200]\ttrain's auc: 0.972596\tvalid's auc: 0.965051\n",
      "[1400]\ttrain's auc: 0.973436\tvalid's auc: 0.965107\n",
      "[1600]\ttrain's auc: 0.974183\tvalid's auc: 0.965142\n",
      "[1800]\ttrain's auc: 0.97485\tvalid's auc: 0.965151\n",
      "[2000]\ttrain's auc: 0.975469\tvalid's auc: 0.965176\n",
      "[2200]\ttrain's auc: 0.976053\tvalid's auc: 0.965172\n",
      "Early stopping, best iteration is:\n",
      "[1963]\ttrain's auc: 0.975362\tvalid's auc: 0.965183\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c03918c11b604ae69112fd3978ccecd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LGBM-gbdt:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022811 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7329\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n",
      "[LightGBM] [Info] Start training from score -1.986273\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[200]\ttrain's auc: 0.962914\tvalid's auc: 0.961574\n",
      "[400]\ttrain's auc: 0.967258\tvalid's auc: 0.964145\n",
      "[600]\ttrain's auc: 0.969623\tvalid's auc: 0.965015\n",
      "[800]\ttrain's auc: 0.971215\tvalid's auc: 0.96537\n",
      "[1000]\ttrain's auc: 0.972475\tvalid's auc: 0.965552\n",
      "[1200]\ttrain's auc: 0.973508\tvalid's auc: 0.965616\n",
      "[1400]\ttrain's auc: 0.974402\tvalid's auc: 0.965631\n",
      "[1600]\ttrain's auc: 0.975212\tvalid's auc: 0.965643\n",
      "Early stopping, best iteration is:\n",
      "[1476]\ttrain's auc: 0.974727\tvalid's auc: 0.965653\n",
      "[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023286 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7261\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n",
      "[LightGBM] [Info] Start training from score -1.986273\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[200]\ttrain's auc: 0.963303\tvalid's auc: 0.960558\n",
      "[400]\ttrain's auc: 0.967576\tvalid's auc: 0.962914\n",
      "[600]\ttrain's auc: 0.969918\tvalid's auc: 0.963676\n",
      "[800]\ttrain's auc: 0.971511\tvalid's auc: 0.963948\n",
      "[1000]\ttrain's auc: 0.972756\tvalid's auc: 0.964106\n",
      "[1200]\ttrain's auc: 0.973788\tvalid's auc: 0.964176\n",
      "[1400]\ttrain's auc: 0.974676\tvalid's auc: 0.964199\n",
      "[1600]\ttrain's auc: 0.975487\tvalid's auc: 0.964187\n",
      "Early stopping, best iteration is:\n",
      "[1361]\ttrain's auc: 0.974514\tvalid's auc: 0.964213\n",
      "[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039898 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7263\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n",
      "[LightGBM] [Info] Start training from score -1.986289\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[200]\ttrain's auc: 0.963206\tvalid's auc: 0.960486\n",
      "[400]\ttrain's auc: 0.967476\tvalid's auc: 0.963057\n",
      "[600]\ttrain's auc: 0.969839\tvalid's auc: 0.963904\n",
      "[800]\ttrain's auc: 0.971424\tvalid's auc: 0.964201\n",
      "[1000]\ttrain's auc: 0.972675\tvalid's auc: 0.964359\n",
      "[1200]\ttrain's auc: 0.973696\tvalid's auc: 0.964406\n",
      "[1400]\ttrain's auc: 0.974607\tvalid's auc: 0.964441\n",
      "[1600]\ttrain's auc: 0.975418\tvalid's auc: 0.964438\n",
      "Early stopping, best iteration is:\n",
      "[1434]\ttrain's auc: 0.97475\tvalid's auc: 0.96445\n",
      "[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022864 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7265\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n",
      "[LightGBM] [Info] Start training from score -1.986289\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[200]\ttrain's auc: 0.963112\tvalid's auc: 0.961279\n",
      "[400]\ttrain's auc: 0.9673\tvalid's auc: 0.963722\n",
      "[600]\ttrain's auc: 0.969633\tvalid's auc: 0.964646\n",
      "[800]\ttrain's auc: 0.97125\tvalid's auc: 0.965058\n",
      "[1000]\ttrain's auc: 0.972491\tvalid's auc: 0.965234\n",
      "[1200]\ttrain's auc: 0.973521\tvalid's auc: 0.965301\n",
      "[1400]\ttrain's auc: 0.974422\tvalid's auc: 0.96536\n",
      "[1600]\ttrain's auc: 0.975223\tvalid's auc: 0.965389\n",
      "[1800]\ttrain's auc: 0.975968\tvalid's auc: 0.965395\n",
      "[2000]\ttrain's auc: 0.976645\tvalid's auc: 0.965365\n",
      "Early stopping, best iteration is:\n",
      "[1814]\ttrain's auc: 0.976025\tvalid's auc: 0.9654\n",
      "[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021301 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7261\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n",
      "[LightGBM] [Info] Start training from score -1.986289\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[200]\ttrain's auc: 0.963256\tvalid's auc: 0.961003\n",
      "[400]\ttrain's auc: 0.96747\tvalid's auc: 0.963444\n",
      "[600]\ttrain's auc: 0.9698\tvalid's auc: 0.964248\n",
      "[800]\ttrain's auc: 0.971385\tvalid's auc: 0.964555\n",
      "[1000]\ttrain's auc: 0.972623\tvalid's auc: 0.964696\n",
      "[1200]\ttrain's auc: 0.973643\tvalid's auc: 0.964741\n",
      "[1400]\ttrain's auc: 0.974525\tvalid's auc: 0.964773\n",
      "[1600]\ttrain's auc: 0.97532\tvalid's auc: 0.964763\n",
      "Early stopping, best iteration is:\n",
      "[1475]\ttrain's auc: 0.974838\tvalid's auc: 0.964783\n",
      "GBDT seeds: 0.9653886686774374 0.9653624253540898 0.964898151194764\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "orig_cat = [\"job\",\"marital\",\"education\",\"default\",\"housing\",\"loan\",\"contact\",\"month\",\"poutcome\"]\n",
    "pair_cat = [\"job_month\",\"contact_month\",\"poutcome_contact\",\"job_contact\",\"education_contact\"]\n",
    "cat_cols_ext = [c for c in orig_cat + pair_cat if c in train_f.columns]\n",
    "\n",
    "bin_like = [\"duration_q50\",\"balance_q20\",\"age_q20\",\"campaign_bin\"]\n",
    "for c in bin_like:\n",
    "    if c in train_f.columns:\n",
    "        train_f[c] = pd.to_numeric(train_f[c], errors=\"coerce\").astype(\"Int32\").fillna(-1).astype(\"int32\")\n",
    "    if c in test_f.columns:\n",
    "        test_f[c] = pd.to_numeric(test_f[c], errors=\"coerce\").astype(\"Int32\").fillna(-1).astype(\"int32\")\n",
    "\n",
    "for c in cat_cols_ext:\n",
    "    if c in train_f.columns: train_f[c] = train_f[c].astype(\"category\")\n",
    "    if c in test_f.columns:  test_f[c]  = test_f[c].astype(\"category\")\n",
    "\n",
    "def align_xy(X_tr, X_te, cats):\n",
    "    for c in cats:\n",
    "        if c in X_tr.columns and c in X_te.columns:\n",
    "            cats_uni = sorted(set(X_tr[c].cat.categories).union(set(X_te[c].cat.categories)))\n",
    "            X_tr[c] = X_tr[c].cat.set_categories(cats_uni)\n",
    "            X_te[c] = X_te[c].cat.set_categories(cats_uni)\n",
    "    tr_only = [c for c in X_tr.columns if c not in X_te.columns]\n",
    "    te_only = [c for c in X_te.columns if c not in X_tr.columns]\n",
    "    for c in tr_only: X_te[c] = 0\n",
    "    for c in te_only: X_tr[c] = 0\n",
    "    X_tr = X_tr[X_te.columns]\n",
    "    return X_tr, X_te\n",
    "\n",
    "y_arr = train_f[\"y\"].values\n",
    "X_all = train_f.drop(columns=[\"y\",\"id\"])\n",
    "X_te_all = test_f.drop(columns=[\"id\"])\n",
    "X_all, X_te_all = align_xy(X_all.copy(), X_te_all.copy(), cat_cols_ext)\n",
    "\n",
    "def cv_lgbm(X_tr_all, y_all, X_te_all, params, n_splits=5, seed=42, num_boost_round=4000, es_rounds=300):\n",
    "    oof = np.zeros(len(X_tr_all))\n",
    "    test_pred = np.zeros(len(X_te_all))\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    folds = list(skf.split(X_tr_all, y_all))\n",
    "    use_es = params.get(\"boosting_type\",\"gbdt\") != \"dart\"\n",
    "    for tr, va in tqdm(folds, desc=f\"LGBM-{params.get('boosting_type','gbdt')}\", leave=False):\n",
    "        X_tr, X_va = X_tr_all.iloc[tr], X_tr_all.iloc[va]\n",
    "        y_tr, y_va = y_all[tr], y_all[va]\n",
    "        dtr = lgb.Dataset(X_tr, label=y_tr, categorical_feature=cat_cols_ext, free_raw_data=False)\n",
    "        dva = lgb.Dataset(X_va, label=y_va, categorical_feature=cat_cols_ext, free_raw_data=False)\n",
    "        cbs = [lgb.log_evaluation(period=200)]\n",
    "        if use_es: cbs.append(lgb.early_stopping(stopping_rounds=es_rounds))\n",
    "        model = lgb.train(params, dtr, valid_sets=[dtr, dva], valid_names=['train','valid'],\n",
    "                          num_boost_round=num_boost_round, callbacks=cbs)\n",
    "        best_iter = model.best_iteration if use_es else num_boost_round\n",
    "        oof[va] = model.predict(X_va, num_iteration=best_iter)\n",
    "        test_pred += model.predict(X_te_all, num_iteration=best_iter) / n_splits\n",
    "    return oof, test_pred\n",
    "\n",
    "pos_weight = (y_arr==0).sum()/(y_arr==1).sum()\n",
    "\n",
    "def make_params(seed, use_pos_weight=False):\n",
    "    p = {\n",
    "        \"objective\":\"binary\",\"metric\":\"auc\",\"boosting_type\":\"gbdt\",\n",
    "        \"learning_rate\":0.02,\"num_leaves\":127,\"min_data_in_leaf\":96,\n",
    "        \"feature_fraction\":0.8,\"bagging_fraction\":0.8,\"bagging_freq\":1,\n",
    "        \"min_sum_hessian_in_leaf\":5.0,\"lambda_l2\":10.0,\"max_bin\":511,\n",
    "        \"extra_trees\":True,\"seed\":seed,\"n_jobs\":-1\n",
    "    }\n",
    "    if use_pos_weight: p[\"scale_pos_weight\"] = pos_weight\n",
    "    return p\n",
    "\n",
    "o1,t1 = cv_lgbm(X_all, y_arr, X_te_all, make_params(42, False), num_boost_round=5000)\n",
    "o2,t2 = cv_lgbm(X_all, y_arr, X_te_all, make_params(7,  False), num_boost_round=5000)\n",
    "o3,t3 = cv_lgbm(X_all, y_arr, X_te_all, make_params(2025, True), num_boost_round=5000)\n",
    "\n",
    "print(\"GBDT seeds:\", roc_auc_score(y_arr,o1), roc_auc_score(y_arr,o2), roc_auc_score(y_arr,o3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "OUT = Path(\"outputs\")\n",
    "SUB = OUT/\"submissions\"\n",
    "CACHE = OUT/\"cache\"\n",
    "for p in [OUT,SUB,CACHE]: p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "data_candidates = [Path.cwd()/ \"data\", Path.cwd().parent/\"data\", Path.cwd().parent.parent/\"data\"]\n",
    "DATA_DIR = next((p for p in data_candidates if (p/\"train.csv\").exists()), None)\n",
    "assert DATA_DIR is not None, \"data/train.csv not found\"\n",
    "\n",
    "train = pd.read_csv(DATA_DIR/\"train.csv\")\n",
    "test  = pd.read_csv(DATA_DIR/\"test.csv\")\n",
    "\n",
    "y = train[\"y\"].values\n",
    "X = train.drop(columns=[\"y\",\"id\"]).copy()\n",
    "X_test = test.drop(columns=[\"id\"]).copy()\n",
    "\n",
    "BASE_NUM = [\"age\",\"balance\",\"day\",\"duration\",\"campaign\",\"pdays\",\"previous\"]\n",
    "BASE_CAT = [\"job\",\"marital\",\"education\",\"default\",\"housing\",\"loan\",\"contact\",\"month\",\"poutcome\"]\n",
    "\n",
    "X_base = train[BASE_NUM + BASE_CAT].copy()\n",
    "X_test_base = test[BASE_NUM + BASE_CAT].copy()\n",
    "\n",
    "for c in BASE_CAT:\n",
    "    X_base[c] = X_base[c].astype(\"category\")\n",
    "    X_test_base[c] = X_test_base[c].astype(\"category\")\n",
    "\n",
    "num_cols = [\"age\",\"balance\",\"day\",\"duration\",\"campaign\",\"pdays\",\"previous\"]\n",
    "cat_cols = [\"job\",\"marital\",\"education\",\"default\",\"housing\",\"loan\",\"contact\",\"month\",\"poutcome\"]\n",
    "\n",
    "for c in cat_cols:\n",
    "    X[c] = X[c].astype(\"category\")\n",
    "    X_test[c] = X_test[c].astype(\"category\")\n",
    "\n",
    "def save_cache(name, oof, pred):\n",
    "    np.save(CACHE/f\"{name}_oof.npy\", oof)\n",
    "    np.save(CACHE/f\"{name}_test.npy\", pred)\n",
    "\n",
    "def load_cache(name):\n",
    "    oof = np.load(CACHE/f\"{name}_oof.npy\")\n",
    "    pred = np.load(CACHE/f\"{name}_test.npy\")\n",
    "    return oof, pred\n",
    "\n",
    "def has_cache(name):\n",
    "    return (CACHE/f\"{name}_oof.npy\").exists() and (CACHE/f\"{name}_test.npy\").exists()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "def _align_lgbm_frames(X_tr_all, X_te_all, cats):\n",
    "    X_tr = X_tr_all.copy()\n",
    "    X_te = X_te_all.copy()\n",
    "    tr_only = [c for c in X_tr.columns if c not in X_te.columns]\n",
    "    te_only = [c for c in X_te.columns if c not in X_tr.columns]\n",
    "    for c in tr_only: X_te[c] = 0\n",
    "    for c in te_only: X_tr[c] = 0\n",
    "    X_tr = X_tr[X_te.columns] \n",
    "\n",
    "    for c in cats:\n",
    "        if c in X_tr.columns and str(X_tr[c].dtype) == \"category\":\n",
    "            cats_union = sorted(set(X_tr[c].cat.categories).union(\n",
    "                               set(X_te[c].cat.categories) if str(X_te[c].dtype)==\"category\" else []))\n",
    "            X_tr[c] = X_tr[c].cat.set_categories(cats_union)\n",
    "            X_te[c] = X_te[c].astype(\"category\").cat.set_categories(cats_union)\n",
    "    return X_tr, X_te\n",
    "\n",
    "def cv_lgbm(Xdf, yarr, Xte, params, n_splits=5, seed=42, num_boost_round=3500, es_rounds=250, cats=BASE_CAT):\n",
    "    X_train_aligned, X_test_aligned = _align_lgbm_frames(Xdf, Xte, cats)\n",
    "    assert list(X_train_aligned.columns) == list(X_test_aligned.columns), \"columns still misaligned\"\n",
    "\n",
    "    oof = np.zeros(len(X_train_aligned))\n",
    "    pred = np.zeros(len(X_test_aligned))\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    folds = list(skf.split(X_train_aligned, yarr))\n",
    "    use_es = params.get(\"boosting_type\",\"gbdt\")!=\"dart\"\n",
    "\n",
    "    for tr, va in tqdm(folds, desc=f\"LGBM-{params.get('boosting_type','gbdt')}\", leave=False):\n",
    "        X_tr, X_va = X_train_aligned.iloc[tr], X_train_aligned.iloc[va]\n",
    "        y_tr, y_va = yarr[tr], yarr[va]\n",
    "        dtr = lgb.Dataset(X_tr, label=y_tr, categorical_feature=cats, free_raw_data=False)\n",
    "        dva = lgb.Dataset(X_va, label=y_va, categorical_feature=cats, free_raw_data=False)\n",
    "        cbs=[lgb.log_evaluation(period=200)]\n",
    "        if use_es: cbs.append(lgb.early_stopping(stopping_rounds=es_rounds))\n",
    "        m = lgb.train(params, dtr, valid_sets=[dtr,dva], valid_names=[\"train\",\"valid\"],\n",
    "                      num_boost_round=num_boost_round, callbacks=cbs)\n",
    "        best_iter = m.best_iteration if use_es else num_boost_round\n",
    "        oof[va] = m.predict(X_va, num_iteration=best_iter)\n",
    "        pred += m.predict(X_test_aligned, num_iteration=best_iter)/n_splits\n",
    "    return oof, pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e192df4ffd948128953f35239f0eb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LGBM-gbdt:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 250 rounds\n",
      "[200]\ttrain's auc: 0.969572\tvalid's auc: 0.967948\n",
      "[400]\ttrain's auc: 0.973677\tvalid's auc: 0.970091\n",
      "[600]\ttrain's auc: 0.976166\tvalid's auc: 0.970833\n",
      "[800]\ttrain's auc: 0.978117\tvalid's auc: 0.971229\n",
      "[1000]\ttrain's auc: 0.979729\tvalid's auc: 0.971425\n",
      "[1200]\ttrain's auc: 0.981158\tvalid's auc: 0.97154\n",
      "[1400]\ttrain's auc: 0.982424\tvalid's auc: 0.971612\n",
      "[1600]\ttrain's auc: 0.983609\tvalid's auc: 0.971647\n",
      "[1800]\ttrain's auc: 0.984694\tvalid's auc: 0.971671\n",
      "[2000]\ttrain's auc: 0.985688\tvalid's auc: 0.971665\n",
      "Early stopping, best iteration is:\n",
      "[1815]\ttrain's auc: 0.984777\tvalid's auc: 0.971679\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "[200]\ttrain's auc: 0.969851\tvalid's auc: 0.966885\n",
      "[400]\ttrain's auc: 0.97401\tvalid's auc: 0.969166\n",
      "[600]\ttrain's auc: 0.976475\tvalid's auc: 0.969914\n",
      "[800]\ttrain's auc: 0.978366\tvalid's auc: 0.970263\n",
      "[1000]\ttrain's auc: 0.979969\tvalid's auc: 0.970426\n",
      "[1200]\ttrain's auc: 0.981398\tvalid's auc: 0.970549\n",
      "[1400]\ttrain's auc: 0.98267\tvalid's auc: 0.970604\n",
      "[1600]\ttrain's auc: 0.983843\tvalid's auc: 0.970634\n",
      "[1800]\ttrain's auc: 0.98491\tvalid's auc: 0.970636\n",
      "Early stopping, best iteration is:\n",
      "[1720]\ttrain's auc: 0.984499\tvalid's auc: 0.970651\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "[200]\ttrain's auc: 0.969903\tvalid's auc: 0.966906\n",
      "[400]\ttrain's auc: 0.974068\tvalid's auc: 0.969106\n",
      "[600]\ttrain's auc: 0.976534\tvalid's auc: 0.969816\n",
      "[800]\ttrain's auc: 0.978424\tvalid's auc: 0.97013\n",
      "[1000]\ttrain's auc: 0.980073\tvalid's auc: 0.970345\n",
      "[1200]\ttrain's auc: 0.981499\tvalid's auc: 0.970445\n",
      "[1400]\ttrain's auc: 0.982789\tvalid's auc: 0.970502\n",
      "[1600]\ttrain's auc: 0.983956\tvalid's auc: 0.970525\n",
      "[1800]\ttrain's auc: 0.985026\tvalid's auc: 0.970516\n",
      "Early stopping, best iteration is:\n",
      "[1652]\ttrain's auc: 0.984238\tvalid's auc: 0.970527\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "[200]\ttrain's auc: 0.969754\tvalid's auc: 0.967795\n",
      "[400]\ttrain's auc: 0.973864\tvalid's auc: 0.969885\n",
      "[600]\ttrain's auc: 0.976339\tvalid's auc: 0.970599\n",
      "[800]\ttrain's auc: 0.978283\tvalid's auc: 0.971002\n",
      "[1000]\ttrain's auc: 0.979918\tvalid's auc: 0.971235\n",
      "[1200]\ttrain's auc: 0.981351\tvalid's auc: 0.971341\n",
      "[1400]\ttrain's auc: 0.982613\tvalid's auc: 0.971396\n",
      "[1600]\ttrain's auc: 0.983788\tvalid's auc: 0.971431\n",
      "[1800]\ttrain's auc: 0.984866\tvalid's auc: 0.971438\n",
      "[2000]\ttrain's auc: 0.985876\tvalid's auc: 0.971452\n",
      "[2200]\ttrain's auc: 0.986781\tvalid's auc: 0.971425\n",
      "Early stopping, best iteration is:\n",
      "[2004]\ttrain's auc: 0.985895\tvalid's auc: 0.971454\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "[200]\ttrain's auc: 0.969794\tvalid's auc: 0.96709\n",
      "[400]\ttrain's auc: 0.973916\tvalid's auc: 0.969346\n",
      "[600]\ttrain's auc: 0.976342\tvalid's auc: 0.970072\n",
      "[800]\ttrain's auc: 0.978241\tvalid's auc: 0.970404\n",
      "[1000]\ttrain's auc: 0.97987\tvalid's auc: 0.970611\n",
      "[1200]\ttrain's auc: 0.981276\tvalid's auc: 0.970764\n",
      "[1400]\ttrain's auc: 0.982552\tvalid's auc: 0.970867\n",
      "[1600]\ttrain's auc: 0.983723\tvalid's auc: 0.970919\n",
      "[1800]\ttrain's auc: 0.984782\tvalid's auc: 0.970953\n",
      "[2000]\ttrain's auc: 0.985773\tvalid's auc: 0.970958\n",
      "Early stopping, best iteration is:\n",
      "[1927]\ttrain's auc: 0.985419\tvalid's auc: 0.970971\n",
      "LGB s42: 0.9710562194647476\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "943cc8b9f779462a8167f667448eadfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LGBM-gbdt:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 250 rounds\n",
      "[200]\ttrain's auc: 0.96957\tvalid's auc: 0.96794\n",
      "[400]\ttrain's auc: 0.973729\tvalid's auc: 0.97014\n",
      "[600]\ttrain's auc: 0.976168\tvalid's auc: 0.97086\n",
      "[800]\ttrain's auc: 0.978107\tvalid's auc: 0.971225\n",
      "[1000]\ttrain's auc: 0.979719\tvalid's auc: 0.971407\n",
      "[1200]\ttrain's auc: 0.981148\tvalid's auc: 0.971525\n",
      "[1400]\ttrain's auc: 0.982435\tvalid's auc: 0.971582\n",
      "[1600]\ttrain's auc: 0.983609\tvalid's auc: 0.971617\n",
      "[1800]\ttrain's auc: 0.984714\tvalid's auc: 0.971648\n",
      "[2000]\ttrain's auc: 0.985719\tvalid's auc: 0.971646\n",
      "[2200]\ttrain's auc: 0.986628\tvalid's auc: 0.971635\n",
      "Early stopping, best iteration is:\n",
      "[1957]\ttrain's auc: 0.985517\tvalid's auc: 0.971653\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "[200]\ttrain's auc: 0.969853\tvalid's auc: 0.966908\n",
      "[400]\ttrain's auc: 0.974023\tvalid's auc: 0.969123\n",
      "[600]\ttrain's auc: 0.97648\tvalid's auc: 0.969891\n",
      "[800]\ttrain's auc: 0.978381\tvalid's auc: 0.970266\n",
      "[1000]\ttrain's auc: 0.979987\tvalid's auc: 0.970471\n",
      "[1200]\ttrain's auc: 0.981397\tvalid's auc: 0.970588\n",
      "[1400]\ttrain's auc: 0.982669\tvalid's auc: 0.970635\n",
      "[1600]\ttrain's auc: 0.983857\tvalid's auc: 0.970654\n",
      "[1800]\ttrain's auc: 0.98492\tvalid's auc: 0.970641\n",
      "Early stopping, best iteration is:\n",
      "[1576]\ttrain's auc: 0.983723\tvalid's auc: 0.97066\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "[200]\ttrain's auc: 0.969851\tvalid's auc: 0.966936\n",
      "[400]\ttrain's auc: 0.974065\tvalid's auc: 0.969227\n",
      "[600]\ttrain's auc: 0.97647\tvalid's auc: 0.969878\n",
      "[800]\ttrain's auc: 0.978372\tvalid's auc: 0.97018\n",
      "[1000]\ttrain's auc: 0.980002\tvalid's auc: 0.970375\n",
      "[1200]\ttrain's auc: 0.981427\tvalid's auc: 0.97049\n",
      "[1400]\ttrain's auc: 0.982725\tvalid's auc: 0.970532\n",
      "[1600]\ttrain's auc: 0.983886\tvalid's auc: 0.970558\n",
      "[1800]\ttrain's auc: 0.984952\tvalid's auc: 0.970568\n",
      "Early stopping, best iteration is:\n",
      "[1692]\ttrain's auc: 0.984381\tvalid's auc: 0.97057\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "[200]\ttrain's auc: 0.969711\tvalid's auc: 0.967801\n",
      "[400]\ttrain's auc: 0.973848\tvalid's auc: 0.970011\n",
      "[600]\ttrain's auc: 0.976277\tvalid's auc: 0.970664\n",
      "[800]\ttrain's auc: 0.978205\tvalid's auc: 0.971027\n",
      "[1000]\ttrain's auc: 0.979821\tvalid's auc: 0.971236\n",
      "[1200]\ttrain's auc: 0.981255\tvalid's auc: 0.971338\n",
      "[1400]\ttrain's auc: 0.982538\tvalid's auc: 0.971392\n",
      "[1600]\ttrain's auc: 0.983697\tvalid's auc: 0.971449\n",
      "[1800]\ttrain's auc: 0.984776\tvalid's auc: 0.971459\n",
      "[2000]\ttrain's auc: 0.985751\tvalid's auc: 0.971459\n",
      "[2200]\ttrain's auc: 0.986662\tvalid's auc: 0.971466\n",
      "Early stopping, best iteration is:\n",
      "[2103]\ttrain's auc: 0.986238\tvalid's auc: 0.971476\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "[200]\ttrain's auc: 0.969877\tvalid's auc: 0.967191\n",
      "[400]\ttrain's auc: 0.973954\tvalid's auc: 0.969381\n",
      "[600]\ttrain's auc: 0.976362\tvalid's auc: 0.970075\n",
      "[800]\ttrain's auc: 0.978291\tvalid's auc: 0.970488\n",
      "[1000]\ttrain's auc: 0.979897\tvalid's auc: 0.9707\n",
      "[1200]\ttrain's auc: 0.981325\tvalid's auc: 0.970854\n",
      "[1400]\ttrain's auc: 0.982588\tvalid's auc: 0.97093\n",
      "[1600]\ttrain's auc: 0.983745\tvalid's auc: 0.97096\n",
      "[1800]\ttrain's auc: 0.984837\tvalid's auc: 0.97099\n",
      "[2000]\ttrain's auc: 0.985824\tvalid's auc: 0.970991\n",
      "Early stopping, best iteration is:\n",
      "[1779]\ttrain's auc: 0.984725\tvalid's auc: 0.970997\n",
      "LGB s7: 0.9710713365963758\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ece37e80df894b6e8910d4a36ea266ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LGBM-gbdt:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 250 rounds\n",
      "[200]\ttrain's auc: 0.96912\tvalid's auc: 0.967158\n",
      "[400]\ttrain's auc: 0.973914\tvalid's auc: 0.969766\n",
      "[600]\ttrain's auc: 0.976571\tvalid's auc: 0.970462\n",
      "[800]\ttrain's auc: 0.978624\tvalid's auc: 0.970826\n",
      "[1000]\ttrain's auc: 0.980364\tvalid's auc: 0.971057\n",
      "[1200]\ttrain's auc: 0.981889\tvalid's auc: 0.971209\n",
      "[1400]\ttrain's auc: 0.983233\tvalid's auc: 0.971301\n",
      "[1600]\ttrain's auc: 0.984453\tvalid's auc: 0.971349\n",
      "[1800]\ttrain's auc: 0.985556\tvalid's auc: 0.971369\n",
      "[2000]\ttrain's auc: 0.986577\tvalid's auc: 0.971376\n",
      "[2200]\ttrain's auc: 0.987519\tvalid's auc: 0.971369\n",
      "Early stopping, best iteration is:\n",
      "[1979]\ttrain's auc: 0.986471\tvalid's auc: 0.971381\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "[200]\ttrain's auc: 0.969518\tvalid's auc: 0.966251\n",
      "[400]\ttrain's auc: 0.974143\tvalid's auc: 0.968669\n",
      "[600]\ttrain's auc: 0.976803\tvalid's auc: 0.969377\n",
      "[800]\ttrain's auc: 0.97888\tvalid's auc: 0.969716\n",
      "[1000]\ttrain's auc: 0.980572\tvalid's auc: 0.969893\n",
      "[1200]\ttrain's auc: 0.982118\tvalid's auc: 0.970013\n",
      "[1400]\ttrain's auc: 0.983469\tvalid's auc: 0.970076\n",
      "[1600]\ttrain's auc: 0.984667\tvalid's auc: 0.970097\n",
      "[1800]\ttrain's auc: 0.985773\tvalid's auc: 0.970137\n",
      "[2000]\ttrain's auc: 0.98676\tvalid's auc: 0.970112\n",
      "Early stopping, best iteration is:\n",
      "[1883]\ttrain's auc: 0.9862\tvalid's auc: 0.970138\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "[200]\ttrain's auc: 0.96947\tvalid's auc: 0.966214\n",
      "[400]\ttrain's auc: 0.974153\tvalid's auc: 0.968608\n",
      "[600]\ttrain's auc: 0.976795\tvalid's auc: 0.969274\n",
      "[800]\ttrain's auc: 0.978838\tvalid's auc: 0.969547\n",
      "[1000]\ttrain's auc: 0.980593\tvalid's auc: 0.969732\n",
      "[1200]\ttrain's auc: 0.982119\tvalid's auc: 0.96983\n",
      "[1400]\ttrain's auc: 0.983465\tvalid's auc: 0.969864\n",
      "[1600]\ttrain's auc: 0.984642\tvalid's auc: 0.969869\n",
      "Early stopping, best iteration is:\n",
      "[1496]\ttrain's auc: 0.984052\tvalid's auc: 0.969879\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "[200]\ttrain's auc: 0.969318\tvalid's auc: 0.967145\n",
      "[400]\ttrain's auc: 0.974032\tvalid's auc: 0.969594\n",
      "[600]\ttrain's auc: 0.976749\tvalid's auc: 0.970375\n",
      "[800]\ttrain's auc: 0.978809\tvalid's auc: 0.970704\n",
      "[1000]\ttrain's auc: 0.980539\tvalid's auc: 0.970928\n",
      "[1200]\ttrain's auc: 0.982052\tvalid's auc: 0.971086\n",
      "[1400]\ttrain's auc: 0.983365\tvalid's auc: 0.971139\n",
      "[1600]\ttrain's auc: 0.984594\tvalid's auc: 0.971176\n",
      "[1800]\ttrain's auc: 0.985729\tvalid's auc: 0.97118\n",
      "[2000]\ttrain's auc: 0.986736\tvalid's auc: 0.971194\n",
      "[2200]\ttrain's auc: 0.98764\tvalid's auc: 0.971187\n",
      "Early stopping, best iteration is:\n",
      "[2060]\ttrain's auc: 0.987003\tvalid's auc: 0.971198\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "[200]\ttrain's auc: 0.96939\tvalid's auc: 0.966421\n",
      "[400]\ttrain's auc: 0.974099\tvalid's auc: 0.968896\n",
      "[600]\ttrain's auc: 0.976737\tvalid's auc: 0.969597\n",
      "[800]\ttrain's auc: 0.978782\tvalid's auc: 0.969948\n",
      "[1000]\ttrain's auc: 0.980513\tvalid's auc: 0.970153\n",
      "[1200]\ttrain's auc: 0.982006\tvalid's auc: 0.970291\n",
      "[1400]\ttrain's auc: 0.983341\tvalid's auc: 0.970388\n",
      "[1600]\ttrain's auc: 0.984562\tvalid's auc: 0.970428\n",
      "[1800]\ttrain's auc: 0.985673\tvalid's auc: 0.970475\n",
      "[2000]\ttrain's auc: 0.986658\tvalid's auc: 0.970474\n",
      "Early stopping, best iteration is:\n",
      "[1935]\ttrain's auc: 0.986348\tvalid's auc: 0.970481\n",
      "LGB spw: 0.9706107428601142\n"
     ]
    }
   ],
   "source": [
    "pos_weight = (y==0).sum()/(y==1).sum()\n",
    "def make_params(seed, spw=None):\n",
    "    p = {\n",
    "        \"objective\":\"binary\",\"metric\":\"auc\",\"boosting_type\":\"gbdt\",\n",
    "        \"learning_rate\":0.03,\"num_leaves\":127,\"min_data_in_leaf\":96,\n",
    "        \"feature_fraction\":0.85,\"bagging_fraction\":0.85,\"bagging_freq\":1,\n",
    "        \"min_sum_hessian_in_leaf\":5.0,\"lambda_l2\":10.0,\"max_bin\":511,\n",
    "        \"seed\":seed,\"n_jobs\":-1,\"verbosity\":-1,\"force_row_wise\":True\n",
    "    }\n",
    "    if spw is not None: p[\"scale_pos_weight\"] = spw\n",
    "    return p\n",
    "\n",
    "if not has_cache(\"lgb_s42\"):\n",
    "    o1,t1 = cv_lgbm(X_base, y, X_test_base, make_params(42), num_boost_round=3200, es_rounds=250, cats=BASE_CAT)\n",
    "    print(\"LGB s42:\", roc_auc_score(y, o1)); save_cache(\"lgb_s42\", o1, t1)\n",
    "\n",
    "if not has_cache(\"lgb_s7\"):\n",
    "    o2,t2 = cv_lgbm(X_base, y, X_test_base, make_params(7), num_boost_round=3200, es_rounds=250, cats=BASE_CAT)\n",
    "    print(\"LGB s7:\", roc_auc_score(y, o2)); save_cache(\"lgb_s7\", o2, t2)\n",
    "\n",
    "if not has_cache(\"lgb_spw\"):\n",
    "    o3,t3 = cv_lgbm(X_base, y, X_test_base, make_params(2025, pos_weight), num_boost_round=3200, es_rounds=250, cats=BASE_CAT)\n",
    "    print(\"LGB spw:\", roc_auc_score(y, o3)); save_cache(\"lgb_spw\", o3, t3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edfa51ebb93f4cc8afddea694af67505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CAT-CPU:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.9147978\tbest: 0.9147978 (0)\ttotal: 384ms\tremaining: 16m\n",
      "200:\ttest: 0.9607437\tbest: 0.9607437 (200)\ttotal: 1m 14s\tremaining: 14m 12s\n",
      "400:\ttest: 0.9637184\tbest: 0.9637184 (399)\ttotal: 2m 54s\tremaining: 15m 13s\n",
      "600:\ttest: 0.9647026\tbest: 0.9647026 (600)\ttotal: 4m 32s\tremaining: 14m 21s\n",
      "800:\ttest: 0.9654050\tbest: 0.9654050 (800)\ttotal: 6m\tremaining: 12m 44s\n",
      "1000:\ttest: 0.9659356\tbest: 0.9659356 (1000)\ttotal: 7m 39s\tremaining: 11m 28s\n",
      "1200:\ttest: 0.9663450\tbest: 0.9663454 (1199)\ttotal: 8m 58s\tremaining: 9m 42s\n",
      "1400:\ttest: 0.9666455\tbest: 0.9666455 (1400)\ttotal: 10m 1s\tremaining: 7m 51s\n",
      "1600:\ttest: 0.9669083\tbest: 0.9669083 (1600)\ttotal: 11m 3s\tremaining: 6m 12s\n",
      "1800:\ttest: 0.9671197\tbest: 0.9671197 (1800)\ttotal: 12m 4s\tremaining: 4m 41s\n",
      "2000:\ttest: 0.9672824\tbest: 0.9672824 (2000)\ttotal: 13m 3s\tremaining: 3m 15s\n",
      "2200:\ttest: 0.9674379\tbest: 0.9674379 (2200)\ttotal: 14m 3s\tremaining: 1m 54s\n",
      "2400:\ttest: 0.9675780\tbest: 0.9675783 (2395)\ttotal: 15m 3s\tremaining: 37.3s\n",
      "2499:\ttest: 0.9676439\tbest: 0.9676439 (2499)\ttotal: 15m 32s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9676438895\n",
      "bestIteration = 2499\n",
      "\n",
      "0:\ttest: 0.9163026\tbest: 0.9163026 (0)\ttotal: 228ms\tremaining: 9m 29s\n",
      "200:\ttest: 0.9599635\tbest: 0.9599635 (200)\ttotal: 56.1s\tremaining: 10m 41s\n",
      "400:\ttest: 0.9626660\tbest: 0.9626660 (400)\ttotal: 1m 54s\tremaining: 9m 58s\n",
      "600:\ttest: 0.9636563\tbest: 0.9636563 (600)\ttotal: 2m 52s\tremaining: 9m 5s\n",
      "800:\ttest: 0.9643619\tbest: 0.9643619 (800)\ttotal: 3m 54s\tremaining: 8m 17s\n",
      "1000:\ttest: 0.9648824\tbest: 0.9648824 (999)\ttotal: 4m 53s\tremaining: 7m 19s\n",
      "1200:\ttest: 0.9652736\tbest: 0.9652736 (1200)\ttotal: 5m 52s\tremaining: 6m 21s\n",
      "1400:\ttest: 0.9655392\tbest: 0.9655392 (1400)\ttotal: 6m 53s\tremaining: 5m 24s\n",
      "1600:\ttest: 0.9657771\tbest: 0.9657771 (1600)\ttotal: 7m 53s\tremaining: 4m 25s\n",
      "1800:\ttest: 0.9659409\tbest: 0.9659409 (1800)\ttotal: 8m 59s\tremaining: 3m 29s\n",
      "2000:\ttest: 0.9661563\tbest: 0.9661563 (2000)\ttotal: 10m 10s\tremaining: 2m 32s\n",
      "2200:\ttest: 0.9662904\tbest: 0.9662904 (2200)\ttotal: 11m 24s\tremaining: 1m 32s\n",
      "2400:\ttest: 0.9664336\tbest: 0.9664336 (2400)\ttotal: 12m 30s\tremaining: 31s\n",
      "2499:\ttest: 0.9664978\tbest: 0.9664978 (2499)\ttotal: 13m 6s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9664978098\n",
      "bestIteration = 2499\n",
      "\n",
      "0:\ttest: 0.9186613\tbest: 0.9186613 (0)\ttotal: 473ms\tremaining: 19m 41s\n",
      "200:\ttest: 0.9598749\tbest: 0.9598749 (200)\ttotal: 1m 13s\tremaining: 13m 56s\n",
      "400:\ttest: 0.9625118\tbest: 0.9625118 (400)\ttotal: 2m 13s\tremaining: 11m 41s\n",
      "600:\ttest: 0.9635707\tbest: 0.9635707 (600)\ttotal: 3m 16s\tremaining: 10m 21s\n",
      "800:\ttest: 0.9641239\tbest: 0.9641239 (799)\ttotal: 4m 32s\tremaining: 9m 38s\n",
      "1000:\ttest: 0.9646271\tbest: 0.9646271 (1000)\ttotal: 5m 51s\tremaining: 8m 46s\n",
      "1200:\ttest: 0.9649757\tbest: 0.9649759 (1199)\ttotal: 7m 24s\tremaining: 8m\n",
      "1400:\ttest: 0.9653556\tbest: 0.9653557 (1399)\ttotal: 9m 11s\tremaining: 7m 12s\n",
      "1600:\ttest: 0.9656370\tbest: 0.9656370 (1600)\ttotal: 10m 46s\tremaining: 6m 3s\n",
      "1800:\ttest: 0.9658400\tbest: 0.9658400 (1800)\ttotal: 12m 17s\tremaining: 4m 46s\n",
      "2000:\ttest: 0.9660275\tbest: 0.9660275 (2000)\ttotal: 14m 4s\tremaining: 3m 30s\n",
      "2200:\ttest: 0.9662032\tbest: 0.9662032 (2200)\ttotal: 15m 41s\tremaining: 2m 7s\n",
      "2400:\ttest: 0.9663397\tbest: 0.9663401 (2393)\ttotal: 17m 9s\tremaining: 42.4s\n",
      "2499:\ttest: 0.9664185\tbest: 0.9664185 (2499)\ttotal: 17m 55s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.966418523\n",
      "bestIteration = 2499\n",
      "\n",
      "0:\ttest: 0.9194612\tbest: 0.9194612 (0)\ttotal: 795ms\tremaining: 33m 7s\n",
      "200:\ttest: 0.9603800\tbest: 0.9603800 (200)\ttotal: 1m 18s\tremaining: 15m 1s\n",
      "400:\ttest: 0.9631729\tbest: 0.9631729 (400)\ttotal: 2m 42s\tremaining: 14m 11s\n",
      "600:\ttest: 0.9644615\tbest: 0.9644615 (600)\ttotal: 3m 56s\tremaining: 12m 27s\n",
      "800:\ttest: 0.9651780\tbest: 0.9651780 (800)\ttotal: 5m 11s\tremaining: 11m 1s\n",
      "1000:\ttest: 0.9656616\tbest: 0.9656616 (1000)\ttotal: 6m 36s\tremaining: 9m 53s\n",
      "1200:\ttest: 0.9660634\tbest: 0.9660634 (1200)\ttotal: 8m\tremaining: 8m 39s\n",
      "1400:\ttest: 0.9663676\tbest: 0.9663676 (1400)\ttotal: 9m 25s\tremaining: 7m 23s\n",
      "1600:\ttest: 0.9666105\tbest: 0.9666105 (1600)\ttotal: 10m 47s\tremaining: 6m 3s\n",
      "1800:\ttest: 0.9668107\tbest: 0.9668107 (1800)\ttotal: 12m 3s\tremaining: 4m 40s\n",
      "2000:\ttest: 0.9670220\tbest: 0.9670220 (1999)\ttotal: 13m 21s\tremaining: 3m 19s\n",
      "2200:\ttest: 0.9671593\tbest: 0.9671593 (2200)\ttotal: 14m 37s\tremaining: 1m 59s\n",
      "2400:\ttest: 0.9672918\tbest: 0.9672918 (2400)\ttotal: 15m 53s\tremaining: 39.3s\n",
      "2499:\ttest: 0.9673721\tbest: 0.9673721 (2499)\ttotal: 16m 30s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9673721347\n",
      "bestIteration = 2499\n",
      "\n",
      "0:\ttest: 0.9172916\tbest: 0.9172916 (0)\ttotal: 416ms\tremaining: 17m 18s\n",
      "200:\ttest: 0.9599603\tbest: 0.9599603 (200)\ttotal: 1m 34s\tremaining: 17m 56s\n",
      "400:\ttest: 0.9629926\tbest: 0.9629926 (400)\ttotal: 3m 3s\tremaining: 16m 2s\n",
      "600:\ttest: 0.9640872\tbest: 0.9640872 (600)\ttotal: 4m 16s\tremaining: 13m 30s\n",
      "800:\ttest: 0.9647203\tbest: 0.9647203 (800)\ttotal: 5m 35s\tremaining: 11m 51s\n",
      "1000:\ttest: 0.9651800\tbest: 0.9651800 (1000)\ttotal: 6m 51s\tremaining: 10m 15s\n",
      "1200:\ttest: 0.9655081\tbest: 0.9655081 (1200)\ttotal: 7m 57s\tremaining: 8m 36s\n",
      "1400:\ttest: 0.9658261\tbest: 0.9658261 (1400)\ttotal: 9m 4s\tremaining: 7m 7s\n",
      "1600:\ttest: 0.9660424\tbest: 0.9660424 (1600)\ttotal: 10m 10s\tremaining: 5m 42s\n",
      "1800:\ttest: 0.9662526\tbest: 0.9662526 (1800)\ttotal: 11m 16s\tremaining: 4m 22s\n",
      "2000:\ttest: 0.9664194\tbest: 0.9664194 (2000)\ttotal: 12m 21s\tremaining: 3m 5s\n",
      "2200:\ttest: 0.9665612\tbest: 0.9665615 (2199)\ttotal: 13m 28s\tremaining: 1m 49s\n",
      "2400:\ttest: 0.9666968\tbest: 0.9666977 (2398)\ttotal: 14m 34s\tremaining: 36.1s\n",
      "2499:\ttest: 0.9667698\tbest: 0.9667700 (2498)\ttotal: 15m 7s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9667700245\n",
      "bestIteration = 2498\n",
      "\n",
      "Shrink model to first 2499 iterations.\n",
      "CAT OOF: 0.966937574891415\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from pathlib import Path\n",
    "\n",
    "def cv_cat_cpu(Xdf, yarr, Xte, cat_cols, n_splits=5, seed=42):\n",
    "    oof = np.zeros(len(Xdf), dtype=float)\n",
    "    pred = np.zeros(len(Xte), dtype=float)\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    for fold, (tr, va) in enumerate(tqdm(list(skf.split(Xdf, yarr)), desc=\"CAT-CPU\", leave=False), 1):\n",
    "        X_tr, X_va = Xdf.iloc[tr], Xdf.iloc[va]\n",
    "        y_tr, y_va = yarr[tr], yarr[va]\n",
    "\n",
    "        trp = Pool(X_tr, y_tr, cat_features=cat_cols)\n",
    "        vap = Pool(X_va, y_va, cat_features=cat_cols)\n",
    "        tep = Pool(Xte,          cat_features=cat_cols)\n",
    "\n",
    "        m = CatBoostClassifier(\n",
    "            iterations=2500,\n",
    "            depth=6,\n",
    "            learning_rate=0.05,\n",
    "            l2_leaf_reg=6,\n",
    "            loss_function=\"Logloss\",\n",
    "            eval_metric=\"AUC\",\n",
    "            random_seed=seed + fold,\n",
    "            od_type=\"Iter\",\n",
    "            od_wait=200,\n",
    "            verbose=200,\n",
    "            task_type=\"CPU\",\n",
    "            thread_count=-1,\n",
    "            allow_writing_files=False,\n",
    "        )\n",
    "        m.fit(trp, eval_set=vap, use_best_model=True)\n",
    "        oof[va] = m.predict_proba(vap)[:, 1]\n",
    "        pred += m.predict_proba(tep)[:, 1] / n_splits\n",
    "\n",
    "    return oof, pred\n",
    "\n",
    "if not has_cache(\"cat_cpu_base\"):\n",
    "    oc, tc = cv_cat_cpu(X_base, y.values if hasattr(y, \"values\") else y, X_test_base, BASE_CAT)\n",
    "    print(\"CAT OOF:\", roc_auc_score(y, oc))\n",
    "    save_cache(\"cat_cpu_base\", oc, tc)\n",
    "\n",
    "Path(\"outputs/submissions\").mkdir(parents=True, exist_ok=True)\n",
    "pd.DataFrame({\"id\": test[\"id\"], \"y\": load_cache(\"cat_cpu_base\")[1]}).to_csv(\n",
    "    \"outputs/submissions/cat_cpu_base.csv\", index=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn: 1.7.1 xgboost: 3.0.4\n",
      "num_inferred: ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n",
      "len(X_base), len(X_test_base): 750000 250000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, xgboost, sklearn\n",
    "print(\"sklearn:\", sklearn.__version__, \"xgboost:\", xgboost.__version__)\n",
    "\n",
    "assert set(BASE_CAT).issubset(set(X_base.columns))\n",
    "num_inferred = [c for c in X_base.columns if c not in BASE_CAT]\n",
    "print(\"num_inferred:\", num_inferred)\n",
    "print(\"len(X_base), len(X_test_base):\", len(X_base), len(X_test_base))\n",
    "\n",
    "for c in BASE_CAT:\n",
    "    if str(X_base[c].dtype) != \"category\":\n",
    "        X_base[c] = X_base[c].astype(\"category\")\n",
    "    if str(X_test_base[c].dtype) != \"category\":\n",
    "        X_test_base[c] = X_test_base[c].astype(\"category\")\n",
    "\n",
    "X_base[num_inferred] = X_base[num_inferred].apply(pd.to_numeric, errors=\"coerce\")\n",
    "X_test_base[num_inferred] = X_test_base[num_inferred].apply(pd.to_numeric, errors=\"coerce\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def _make_ohe():\n",
    "    try:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "def cv_xgb_ohe_native(Xdf, yarr, Xte, base_cat, n_splits=5, seed=42, es_rounds=400, num_boost_round=6000):\n",
    "    num_base = [c for c in Xdf.columns if c not in base_cat]\n",
    "    oof = np.zeros(len(Xdf), dtype=float)\n",
    "    pred = np.zeros(len(Xte), dtype=float)\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "    for fold, (tr, va) in enumerate(tqdm(list(skf.split(Xdf, yarr)), desc=\"XGB-OHE(native)\", leave=False), 1):\n",
    "        X_tr, X_va = Xdf.iloc[tr].copy(), Xdf.iloc[va].copy()\n",
    "        y_tr, y_va = yarr[tr], yarr[va]\n",
    "        X_te = Xte.copy()\n",
    "\n",
    "        ohe = _make_ohe()\n",
    "        Xtr = np.hstack([X_tr[num_base].to_numpy(dtype=np.float32),\n",
    "                         ohe.fit_transform(X_tr[base_cat].astype(str)).astype(np.float32)])\n",
    "        Xva = np.hstack([X_va[num_base].to_numpy(dtype=np.float32),\n",
    "                         ohe.transform(X_va[base_cat].astype(str)).astype(np.float32)])\n",
    "        Xtt = np.hstack([X_te[num_base].to_numpy(dtype=np.float32),\n",
    "                         ohe.transform(X_te[base_cat].astype(str)).astype(np.float32)])\n",
    "\n",
    "        dtr = xgb.DMatrix(Xtr, label=y_tr)\n",
    "        dva = xgb.DMatrix(Xva, label=y_va)\n",
    "        dte = xgb.DMatrix(Xtt)\n",
    "\n",
    "        params = {\n",
    "            \"objective\": \"binary:logistic\",\n",
    "            \"eval_metric\": \"auc\",\n",
    "            \"eta\": 0.03,\n",
    "            \"max_depth\": 6,\n",
    "            \"subsample\": 0.8,\n",
    "            \"colsample_bytree\": 0.8,\n",
    "            \"lambda\": 5.0,\n",
    "            \"alpha\": 0.0,\n",
    "            \"tree_method\": \"hist\",\n",
    "            \"seed\": seed + fold,\n",
    "        }\n",
    "\n",
    "        es = xgb.callback.EarlyStopping(rounds=es_rounds, save_best=True, maximize=True)\n",
    "        bst = xgb.train(params, dtr, num_boost_round=num_boost_round,\n",
    "                        evals=[(dtr, \"train\"), (dva, \"valid\")],\n",
    "                        callbacks=[es], verbose_eval=False)\n",
    "\n",
    "        oof[va] = bst.predict(dva)\n",
    "        pred += bst.predict(dte) / n_splits\n",
    "\n",
    "    return oof, pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b38c372cf0054e919b1426d2664ebd87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "XGB-OHE(native):   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB(OHE) OOF: 0.968435070082612\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "if not has_cache(\"xgb_ohe_base\"):\n",
    "    ox, tx = cv_xgb_ohe_native(X_base, y, X_test_base, BASE_CAT, n_splits=5, seed=42, es_rounds=400, num_boost_round=6000)\n",
    "    print(\"XGB(OHE) OOF:\", roc_auc_score(y, ox))\n",
    "    save_cache(\"xgb_ohe_base\", ox, tx)\n",
    "else:\n",
    "    ox, tx = load_cache(\"xgb_ohe_base\")\n",
    "    print(\"XGB(OHE) OOF:\", roc_auc_score(y, ox))\n",
    "\n",
    "pd.DataFrame({\"id\": test[\"id\"], \"y\": tx}).to_csv(\"outputs/submissions/xgb_ohe_base.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
