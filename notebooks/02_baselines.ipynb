{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a358920a",
   "metadata": {},
   "source": [
    "# 02 â€¢ Baselines and Stacking (reproducible from cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16caf1b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEmptyDataError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m BASE_CAT = [\u001b[33m'\u001b[39m\u001b[33mjob\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mmarital\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33meducation\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mhousing\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mloan\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mcontact\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mmonth\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mpoutcome\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     22\u001b[39m train = pd.read_csv(DATA/\u001b[33m'\u001b[39m\u001b[33mtrain.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m test  = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA\u001b[49m\u001b[43m/\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtest.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m y = train[\u001b[33m'\u001b[39m\u001b[33my\u001b[39m\u001b[33m'\u001b[39m].values\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/kaggle-bankterm/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/kaggle-bankterm/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/kaggle-bankterm/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/kaggle-bankterm/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1898\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/kaggle-bankterm/.venv/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[39m, in \u001b[36mCParserWrapper.__init__\u001b[39m\u001b[34m(self, src, **kwds)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[33m\"\u001b[39m\u001b[33mdtype_backend\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[32m     92\u001b[39m     import_optional_dependency(\u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28mself\u001b[39m._reader = \u001b[43mparsers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.unnamed_cols = \u001b[38;5;28mself\u001b[39m._reader.unnamed_cols\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:581\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.__cinit__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mEmptyDataError\u001b[39m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os, numpy as np, pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from datetime import datetime\n",
    "\n",
    "PROJ = Path.cwd()\n",
    "DATA = None\n",
    "for p in [PROJ/'data', PROJ.parent/'data', PROJ.parent.parent/'data']:\n",
    "    if (p/'train.csv').exists() and (p/'test.csv').exists():\n",
    "        DATA = p; break\n",
    "assert DATA is not None, 'data/train.csv or test.csv not found'\n",
    "\n",
    "OUT = PROJ/'notebooks'/'outputs'\n",
    "CACHE = OUT/'cache'\n",
    "SUB = OUT/'submissions'\n",
    "for d in [OUT, CACHE, SUB]: d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BASE_NUM = ['age','balance','day','duration','campaign','pdays','previous']\n",
    "BASE_CAT = ['job','marital','education','default','housing','loan','contact','month','poutcome']\n",
    "\n",
    "train = pd.read_csv(DATA/'train.csv')\n",
    "test  = pd.read_csv(DATA/'test.csv')\n",
    "y = train['y'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5fd8cc",
   "metadata": {},
   "source": [
    "## Feature engineering used in final blend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade894dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def month_to_num(m):\n",
    "    d={'jan':1,'feb':2,'mar':3,'apr':4,'may':5,'jun':6,'jul':7,'aug':8,'sep':9,'oct':10,'nov':11,'dec':12}\n",
    "    return d.get(m,0)\n",
    "\n",
    "def make_curated_features(df):\n",
    "    out = df.copy()\n",
    "    out['duration_clip_99'] = np.minimum(out['duration'], out['duration'].quantile(0.99))\n",
    "    out['duration_log1p'] = np.log1p(out['duration_clip_99'])\n",
    "    out['duration_per_call'] = out['duration_clip_99'] / (out['campaign'] + 1.0)\n",
    "    out['pdays_was_contacted'] = (out['pdays'] != -1).astype(int)\n",
    "    out['pdays_pos_log'] = np.log1p(out['pdays'].where(out['pdays'] != -1, np.nan)).fillna(0.0)\n",
    "    out['previous_gt0'] = (out['previous'] > 0).astype(int)\n",
    "    out['month_num'] = out['month'].map(month_to_num).astype(int)\n",
    "    out['month_sin'] = np.sin(2*np.pi*out['month_num']/12.0)\n",
    "    out['month_cos'] = np.cos(2*np.pi*out['month_num']/12.0)\n",
    "    out['day_sin'] = np.sin(2*np.pi*out['day']/31.0)\n",
    "    out['day_cos'] = np.cos(2*np.pi*out['day']/31.0)\n",
    "    out['contact_cellular'] = (out['contact'] == 'cellular').astype(int)\n",
    "    out['dur_x_cell'] = out['duration_clip_99'] * out['contact_cellular']\n",
    "    return out\n",
    "\n",
    "train_f = make_curated_features(train.drop(columns=['id']))\n",
    "test_f  = make_curated_features(test.drop(columns=['id']))\n",
    "X_feat = train_f.drop(columns=['y']).copy()\n",
    "X_test_feat = test_f.copy()\n",
    "for c in BASE_CAT:\n",
    "    if c in X_feat.columns: X_feat[c] = X_feat[c].astype('category')\n",
    "    if c in X_test_feat.columns: X_test_feat[c] = X_test_feat[c].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e98a9e",
   "metadata": {},
   "source": [
    "## CV helpers and cache IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a8a663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cache(name, oof, pred, cache_dir=None):\n",
    "    cache_dir = (cache_dir or (Path.cwd()/'notebooks'/'outputs'/'cache'))\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    np.save(cache_dir/f\"{name}_oof.npy\", oof)\n",
    "    np.save(cache_dir/f\"{name}_test.npy\", pred)\n",
    "\n",
    "def load_cache(name, cache_dir=None):\n",
    "    cache_dir = (cache_dir or (Path.cwd()/'notebooks'/'outputs'/'cache'))\n",
    "    o = np.load(cache_dir/f\"{name}_oof.npy\")\n",
    "    t = np.load(cache_dir/f\"{name}_test.npy\")\n",
    "    return o, t\n",
    "\n",
    "def has_cache(name, cache_dir=None):\n",
    "    cache_dir = (cache_dir or (Path.cwd()/'notebooks'/'outputs'/'cache'))\n",
    "    return (cache_dir/f\"{name}_oof.npy\").exists() and (cache_dir/f\"{name}_test.npy\").exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49b96ad",
   "metadata": {},
   "source": [
    "## LightGBM (3 seeds + pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46619ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def cv_lgbm(X_tr_all, y_all, X_te_all, params, n_splits=5, seed=42, num_boost_round=3500, es_rounds=250, cats=BASE_CAT):\n",
    "    oof = np.zeros(len(X_tr_all)); pred = np.zeros(len(X_te_all))\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    use_es = params.get('boosting_type','gbdt')!='dart'\n",
    "    for tr, va in skf.split(X_tr_all, y_all):\n",
    "        X_tr, X_va = X_tr_all.iloc[tr], X_tr_all.iloc[va]\n",
    "        y_tr, y_va = y_all[tr], y_all[va]\n",
    "        dtr = lgb.Dataset(X_tr, label=y_tr, categorical_feature=cats, free_raw_data=False)\n",
    "        dva = lgb.Dataset(X_va, label=y_va, categorical_feature=cats, free_raw_data=False)\n",
    "        cbs=[lgb.log_evaluation(period=200)]\n",
    "        if use_es: cbs.append(lgb.early_stopping(stopping_rounds=es_rounds))\n",
    "        m = lgb.train(params, dtr, valid_sets=[dtr,dva], valid_names=['train','valid'],\n",
    "                      num_boost_round=num_boost_round, callbacks=cbs)\n",
    "        best_iter = m.best_iteration if use_es else num_boost_round\n",
    "        oof[va] = m.predict(X_va, num_iteration=best_iter)\n",
    "        pred += m.predict(X_te_all, num_iteration=best_iter)/n_splits\n",
    "    return oof, pred\n",
    "\n",
    "pos_weight = (y==0).sum()/(y==1).sum()\n",
    "def make_params(seed, spw=None):\n",
    "    p = {\n",
    "        'objective':'binary','metric':'auc','boosting_type':'gbdt',\n",
    "        'learning_rate':0.03,'num_leaves':127,'min_data_in_leaf':96,\n",
    "        'feature_fraction':0.85,'bagging_fraction':0.85,'bagging_freq':1,\n",
    "        'min_sum_hessian_in_leaf':5.0,'lambda_l2':10.0,'max_bin':511,\n",
    "        'seed':seed,'n_jobs':-1,'verbosity':-1,'force_row_wise':True\n",
    "    }\n",
    "    if spw is not None: p['scale_pos_weight'] = spw\n",
    "    return p\n",
    "\n",
    "if not has_cache('lgbF_s42'):\n",
    "    o1,t1 = cv_lgbm(X_feat, y, X_test_feat, make_params(42), num_boost_round=3200, es_rounds=250)\n",
    "    save_cache('lgbF_s42', o1, t1)\n",
    "if not has_cache('lgbF_s7'):\n",
    "    o2,t2 = cv_lgbm(X_feat, y, X_test_feat, make_params(7), num_boost_round=3200, es_rounds=250)\n",
    "    save_cache('lgbF_s7', o2, t2)\n",
    "if not has_cache('lgbF_spw'):\n",
    "    o3,t3 = cv_lgbm(X_feat, y, X_test_feat, make_params(2025, pos_weight), num_boost_round=3200, es_rounds=250)\n",
    "    save_cache('lgbF_spw', o3, t3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c779cffd",
   "metadata": {},
   "source": [
    "## CatBoost (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e3d2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "def cv_cat_cpu_feat(Xdf, yarr, Xte, cat_cols, n_splits=5, seed=42):\n",
    "    oof = np.zeros(len(Xdf)); pred = np.zeros(len(Xte))\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    for fold,(tr,va) in enumerate(skf.split(Xdf,yarr),1):\n",
    "        X_tr, X_va = Xdf.iloc[tr], Xdf.iloc[va]\n",
    "        y_tr, y_va = yarr[tr], yarr[va]\n",
    "        trp = Pool(X_tr, y_tr, cat_features=cat_cols)\n",
    "        vap = Pool(X_va, y_va, cat_features=cat_cols)\n",
    "        tep = Pool(Xte,           cat_features=cat_cols)\n",
    "        m = CatBoostClassifier(iterations=2500, depth=6, learning_rate=0.05, l2_leaf_reg=6,\n",
    "                               loss_function='Logloss', eval_metric='AUC', random_seed=seed+fold,\n",
    "                               od_type='Iter', od_wait=200, verbose=200, task_type='CPU',\n",
    "                               thread_count=-1, allow_writing_files=False)\n",
    "        m.fit(trp, eval_set=vap, use_best_model=True)\n",
    "        oof[va] = m.predict_proba(vap)[:,1]\n",
    "        pred += m.predict_proba(tep)[:,1]/n_splits\n",
    "    return oof, pred\n",
    "\n",
    "if not has_cache('catF_cpu'):\n",
    "    oc, tc = cv_cat_cpu_feat(X_feat, y, X_test_feat, BASE_CAT)\n",
    "    save_cache('catF_cpu', oc, tc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d20bd35",
   "metadata": {},
   "source": [
    "## XGBoost with OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2308c2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def _make_ohe():\n",
    "    try: return OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    except TypeError: return OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "def cv_xgb_ohe_native(Xdf, yarr, Xte, base_cat, n_splits=5, seed=42, es_rounds=400, num_boost_round=6000):\n",
    "    num_base = [c for c in Xdf.columns if c not in base_cat]\n",
    "    oof = np.zeros(len(Xdf)); pred = np.zeros(len(Xte))\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    for fold,(tr,va) in enumerate(skf.split(Xdf,yarr),1):\n",
    "        X_tr, X_va = Xdf.iloc[tr].copy(), Xdf.iloc[va].copy()\n",
    "        y_tr, y_va = yarr[tr], yarr[va]\n",
    "        X_te = Xte.copy()\n",
    "        ohe = _make_ohe()\n",
    "        Xtr = np.hstack([X_tr[num_base].to_numpy(dtype=np.float32),\n",
    "                         ohe.fit_transform(X_tr[base_cat].astype(str)).astype(np.float32)])\n",
    "        Xva = np.hstack([X_va[num_base].to_numpy(dtype=np.float32),\n",
    "                         ohe.transform(X_va[base_cat].astype(str)).astype(np.float32)])\n",
    "        Xtt = np.hstack([X_te[num_base].to_numpy(dtype=np.float32),\n",
    "                         ohe.transform(X_te[base_cat].astype(str)).astype(np.float32)])\n",
    "        dtr = xgb.DMatrix(Xtr, label=y_tr); dva = xgb.DMatrix(Xva, label=y_va); dte = xgb.DMatrix(Xtt)\n",
    "        params = {'objective':'binary:logistic','eval_metric':'auc','eta':0.03,'max_depth':6,\n",
    "                  'subsample':0.8,'colsample_bytree':0.8,'lambda':5.0,'alpha':0.0,\n",
    "                  'tree_method':'hist','seed':seed+fold}\n",
    "        es = xgb.callback.EarlyStopping(rounds=es_rounds, save_best=True, maximize=True)\n",
    "        bst = xgb.train(params, dtr, num_boost_round=num_boost_round, evals=[(dtr,'train'),(dva,'valid')],\n",
    "                        callbacks=[es], verbose_eval=False)\n",
    "        oof[va] = bst.predict(dva)\n",
    "        pred += bst.predict(dte)/n_splits\n",
    "    return oof, pred\n",
    "\n",
    "if not has_cache('xgbF_ohe'):\n",
    "    ox, tx = cv_xgb_ohe_native(X_feat, y, X_test_feat, BASE_CAT, n_splits=5, seed=42, es_rounds=400, num_boost_round=6000)\n",
    "    save_cache('xgbF_ohe', ox, tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcbc14b",
   "metadata": {},
   "source": [
    "## Report OOF AUC per model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452be07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "o1,_=load_cache('lgbF_s42'); o2,_=load_cache('lgbF_s7'); o3,_=load_cache('lgbF_spw'); oc,_=load_cache('catF_cpu'); ox,_=load_cache('xgbF_ohe')\n",
    "print({\n",
    "    'lgbF_s42': float(roc_auc_score(y,o1)),\n",
    "    'lgbF_s7': float(roc_auc_score(y,o2)),\n",
    "    'lgbF_spw': float(roc_auc_score(y,o3)),\n",
    "    'catF_cpu': float(roc_auc_score(y,oc)),\n",
    "    'xgbF_ohe': float(roc_auc_score(y,ox)),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8652fff7",
   "metadata": {},
   "source": [
    "## Stacking (Logistic Regression) and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f987f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "o1,t1 = load_cache('lgbF_s42')\n",
    "o2,t2 = load_cache('lgbF_s7')\n",
    "o3,t3 = load_cache('lgbF_spw')\n",
    "oc,tc = load_cache('catF_cpu')\n",
    "ox,tx = load_cache('xgbF_ohe')\n",
    "\n",
    "Z_tr = np.vstack([o1,o2,o3,oc,ox]).T\n",
    "Z_te = np.vstack([t1,t2,t3,tc,tx]).T\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_meta = np.zeros(len(y)); pred_meta = np.zeros(len(test))\n",
    "for tr, va in skf.split(Z_tr, y):\n",
    "    m = LogisticRegression(max_iter=1000)\n",
    "    m.fit(Z_tr[tr], y[tr])\n",
    "    oof_meta[va] = m.predict_proba(Z_tr[va])[:,1]\n",
    "    pred_meta += m.predict_proba(Z_te)[:,1]/skf.n_splits\n",
    "\n",
    "print('Stack OOF AUC:', float(roc_auc_score(y, oof_meta)))\n",
    "ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "sub_path = (PROJ/'notebooks'/'outputs'/'submissions'/f'final_stack_{ts}.csv')\n",
    "pd.DataFrame({'id': pd.read_csv(DATA/'test.csv')['id'], 'y': pred_meta}).to_csv(sub_path, index=False)\n",
    "print('Saved:', sub_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
